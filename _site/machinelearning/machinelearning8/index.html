<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>[Machine learning] 8. 확률적 경사 하강법 - Wonny’s DevLog</title>
<meta name="description" content="본 포스팅은 “혼자 공부하는 머신러닝+딥러닝” 책 내용을 기반으로 작성되었습니다. 잘못된 내용이 있을 경우 지적해 주시면 감사드리겠습니다.">


  <meta name="author" content="K.W. Yang">
  
  <meta property="article:author" content="K.W. Yang">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Wonny's DevLog">
<meta property="og:title" content="[Machine learning] 8. 확률적 경사 하강법">
<meta property="og:url" content="http://localhost:4000/machinelearning/machinelearning8/">


  <meta property="og:description" content="본 포스팅은 “혼자 공부하는 머신러닝+딥러닝” 책 내용을 기반으로 작성되었습니다. 잘못된 내용이 있을 경우 지적해 주시면 감사드리겠습니다.">







  <meta property="article:published_time" content="2022-02-15T00:00:00+09:00">





  

  


<link rel="canonical" href="http://localhost:4000/machinelearning/machinelearning8/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Kiwon Yang",
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Wonny's DevLog Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    TeX: {
        equationNumbers: {
        autoNumber: "AMS"
        }
    },
    tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$'] ],
    processEscapes: true,
    }
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
        alert("Math Processing Error: "+message[1]);
    });
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
        alert("Math Processing Error: "+message[1]);
    });
</script>
<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/assets/images/logo.png" alt="Wonny's DevLog"></a>
        
        <a class="site-title" href="/">
          Wonny's DevLog
          <span class="site-subtitle">Version 1.0</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/">Category</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/profile.jpg" alt="K.W. Yang" itemprop="image" class="u-photo">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">K.W. Yang</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>AI Researcher &amp; Developer</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">South Korea</span>
        </li>
      

      
        
          
        
          
        
          
        
          
        
          
            <li><a href="https://github.com/kiwon107" rel="nofollow noopener noreferrer me"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
            <li><a href="https://instagram.com/k1_won" rel="nofollow noopener noreferrer me"><i class="fab fa-fw fa-instagram" aria-hidden="true"></i><span class="label">Instagram</span></a></li>
          
        
      

      

      
        <li>
          <a href="mailto:p2881p@naver.com" rel="me" class="u-email">
            <meta itemprop="email" content="p2881p@naver.com" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span>
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>
  
    
      
      
      
      
    
    
      

<nav class="nav__list">
  
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">Toggle menu</label>
  <ul class="nav__items">
    
      <li>
        
          <span class="nav__sub-title">AI</span>
        

        
        <ul>
          
          
            <li><a href="/machinelearning/">머신러닝(사이킷런) (13)</a></li>
          
          
            <li><a href="/deeplearningpyt/">딥러닝(파이토치) (8)</a></li>
          
          
            <li><a href="/deeplearningtens/">딥러닝(텐서플로우) ()</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          <span class="nav__sub-title">Language</span>
        

        
        <ul>
          
          
            <li><a href="/pythonmd/">파이썬 중급 (26)</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          <span class="nav__sub-title">CS Theory</span>
        

        
        <ul>
          
          
            <li><a href="/os/">운영체제 ()</a></li>
          
          
            <li><a href="/algopy/">자료구조와 알고리즘(파이썬) (1)</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          <span class="nav__sub-title">Mathematics</span>
        

        
        <ul>
          
          
            <li><a href="/prob/">확률과 통계 (2)</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          <a href="/paper/"><span class="nav__sub-title">논문</span></a>
        

        
      </li>
    
  </ul>
</nav>

    
  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="[Machine learning] 8. 확률적 경사 하강법">
    <meta itemprop="description" content="본 포스팅은 “혼자 공부하는 머신러닝+딥러닝” 책 내용을 기반으로 작성되었습니다.잘못된 내용이 있을 경우 지적해 주시면 감사드리겠습니다.">
    <meta itemprop="datePublished" content="2022-02-15T00:00:00+09:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="http://localhost:4000/machinelearning/machinelearning8/" class="u-url" itemprop="url">[Machine learning] 8. 확률적 경사 하강법
</a>
          </h1>
          

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2022-02-15T00:00:00+09:00">February 15, 2022</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          5 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content e-content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#8-1-점진적인-학습">8-1. 점진적인 학습</a></li><li><a href="#8-2-확률적-경사-하강법">8-2. 확률적 경사 하강법</a></li><li><a href="#8-3-손실-함수">8-3. 손실 함수</a></li><li><a href="#8-4-로지스틱-손실-함수">8-4. 로지스틱 손실 함수</a></li><li><a href="#8-5-sgdclassifier">8-5. SGDClassifier</a></li><li><a href="#8-6-에포크와-과대과소적합">8-6. 에포크와 과대/과소적합</a></li><li><a href="#8-7-힌지-손실--서포트-벡터-머신">8-7. 힌지 손실 / 서포트 벡터 머신</a></li></ul>

            </nav>
          </aside>
        
        <p>본 포스팅은 “혼자 공부하는 머신러닝+딥러닝” 책 내용을 기반으로 작성되었습니다.
잘못된 내용이 있을 경우 지적해 주시면 감사드리겠습니다.</p>

<h2 id="8-1-점진적인-학습">8-1. 점진적인 학습</h2>
<p>훈련데이터가 항상 한 번에 준비되면 너무 좋을 것이다. 그러나 현실은 녹록치 않다. 만약 데이터가 조금씩 꾸준히 전달되는 경우라면 어떻게 해야할까?<br />
새로운 데이터를 추가할 때 마다 이전 데이터를 버려서 훈련 데이터 크기를 일정하게 유지한다면? 이 경우, 버린 데이터 안에 중요한 데이터가 들어있다면 큰일이 아닐 수 없다. 데이터를 버리지 않고 새로운 데이터에 대해 조금씩 더 훈련하는 방법! 이것이 바로 <strong>점진적 학습</strong> 또는 <strong>온라인 학습</strong>이라고 한다. 대표적인 점진적 학습 알고리즘은 <strong>확률적 경사 하강법(Stochastic Gradient Descent)</strong>이다.</p>

<h2 id="8-2-확률적-경사-하강법">8-2. 확률적 경사 하강법</h2>
<p>경사는 기울기를 말한다. 하강법은 내려가는 방법을 말한다. 즉, 경사 하강법은 경사를 따라 내려가는 방법을 말한다. 경사 하강법에서 중요한 것은, 가장 가파른 길을 찾아서 조금씩 내려오는 것이다. 한번에 걸음이 너무 크면, 경사를 따라 내려가지 못하고 오히려 올라가는 경우가 발생할 수 있기 때문이다.<br />
그럼 확률적이란 말은 무엇일까? 훈련 세트를 사용하여 모델을 훈련하므로, 경사 하강법도 당연히 훈련 세트를 사용해서 가장 가파른 길을 찾는다. 이때, 전체 샘플을 사용하지 않고 딱 하나의 샘플을 훈련 세트에서 랜덤하게 골라 가장 가파른 깅를 찾는 것도 한가지 방법이 될 수 있다. 이것이 바로 <strong>확률적 경사 하강법</strong>이다!</p>

<p><strong>확률적 경사 하강법</strong>은 훈련 세트에서 랜덤하게 하나의 샘플을 선택하여 가파른 경사를 조금 내려간다. 그 다음 훈련세트에서 랜덤하게 또 다른 샘플을 하나 선택하여 경사를 조금 내려간다. 이런식으로 전체 샘플을 모두 사용할 때까지 계속한다. 모든 샘플을 다 사용했는데, 산을 못내려왔다면? 그럼, 훈련 세트의 모든 샘플을 다시 채워 넣는다. 그리고 다시 랜덤하게 하나의 샘플을 선택하여 계속 경사를 내려간다. 이렇게 훈련 세트를 한 번 모두 사용하는 과정을 <strong>에포크(Epoch)</strong> 라고 부른다. 보통 경사 하강법은 수십, 수백번 이상 에포크를 수행한다.</p>

<p>무작위로 샘플을 선택해서 산을 내려가는데, 과연 잘 찾아갈 수 있을까? 확률적 경사 하강법은 꽤 잘 작동한다. 하나의 샘플보다 여러 개의 샘플을 사용해서 경사 하강법을 수행하는 방식도 있다. 이러한 방식을 <strong>미니배치 경사 하강법(Minibatch gradient descent)</strong> 라고 한다. 실전에서 가장 많이 사용하는 방법이다.</p>

<p>극단적으로 한 번 경사로를 따라 이동하기 위해 전체 샘플을 사용하는 경우도 있다. 이를 <strong>배치 경사 하강법(Batch gradient descent)</strong> 라고 부른다. 전체 데이터를 사용하므로 가장 안정적이나, 그만큼 컴퓨터 자원을 많이 사용하게 된다. 심지어, 메모리가 무족해서 한 번에 전체 데이터를 모두 읽을 수 없는 경우도 발생한다.</p>

<p>이제 훈련 데이터가 모두 준비되어 있지 않고 매일매일 업데이트 되는 방식으로 제공되어도, 계속 학습을 이어나갈 수 있다. 즉, 산꼭대기에서부터 다시 시작할 필요가 없는 것이다!</p>

<p>이제 경사 하강법은 이해했다. 그렇다면 어떻게, 어디서 가파른 길을 찾아 내려가야할까? 바로 손실함수를 통해 알 수 있다!</p>

<h2 id="8-3-손실-함수">8-3. 손실 함수</h2>
<p><strong>손실 함수(Loss function)</strong>는 머신러닝 알고리즘이 어떤 문제에 대해 얼마나 엉터리인지를 측정하는 기준이다! 이 손실 함수 값은 작을수록 좋다. 이 최적의 손실 함수 값을 찾기 위해 가파른 길로 산을 조금씩 이동하는 것이라 보면 된다. 참고로 손실 함수는 샘플 하나에 대한 손실을 의미한다. 반면, 비용 함수(Cost function)는 훈련 세트에 있는 모든 샘플에 대한 손실 함수의 합을 말한다.</p>

<p>이제 분류 기준에서 손실함수를 바라보자. 분류에서 손실함수는 정답을 못맞히는 것이다. 맞혔다, 못맞혔다 2가지 경우의 수로 손실함수를 정할 수는 없다. 경사 하강법으로 조금씩 산을 내려가려면 산의 경사면이 연속적이어야 한다! 그럼 손실 함수도 연속적이여야 한다는 의미가 된다! 맞혔다, 못맞혔다를 확률 0~1 사이의 값으로 나타내기 위해 <strong>로지스틱 손실 함수</strong>를 이용할 수 있다!</p>

<h2 id="8-4-로지스틱-손실-함수">8-4. 로지스틱 손실 함수</h2>
<p><strong>로지스틱 손실 함수(Logistic loss function)</strong>는 다음과 같이 정의된다.
타깃=1 일 때, $로지스틱 손실 함수 = -log(예측 확률)$<br />
타깃=0 일 때, $로지스틱 손실 함수 = -log(1-예측 확률)$<br />
양성 클래스(타깃=1)일 때, 확률이 1에서 멀어질수록 손실은 아주 큰 양수가 된다. 그리고 음성 클래스(타깃=0)일 때, 예측 확률이 0에서 멀어질수록 손실은 아주 큰 양수가 된다.
다중 분류도 매우 비슷한 손실 함수를 사용한다. 다중 분류에서 사용하는 손실 함수를 <strong>크로스엔트로피 손실 함수(Cross-entropy loss function)</strong> 라고 부른다.<br />
참고로, 회귀의 손실 함수로는, 평균 절대값 오차 또는 평균 제곱 오차를 많이 사용한다.</p>

<h2 id="8-5-sgdclassifier">8-5. SGDClassifier</h2>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="n">fish</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'https://bit.ly/fish_csv_data'</span><span class="p">)</span>
<span class="n">fish_input</span> <span class="o">=</span> <span class="n">fish</span><span class="p">[[</span><span class="s">'Weight'</span><span class="p">,</span> <span class="s">'Length'</span><span class="p">,</span> <span class="s">'Diagonal'</span><span class="p">,</span> <span class="s">'Height'</span><span class="p">,</span> <span class="s">'Width'</span><span class="p">]].</span><span class="n">to_numpy</span><span class="p">()</span>
<span class="n">fish_target</span> <span class="o">=</span> <span class="n">fish</span><span class="p">[</span><span class="s">'Species'</span><span class="p">].</span><span class="n">to_numpy</span><span class="p">()</span>

<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">train_input</span><span class="p">,</span> <span class="n">test_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">test_target</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">fish_input</span><span class="p">,</span> <span class="n">fish_target</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="n">ss</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">ss</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_input</span><span class="p">)</span>
<span class="n">train_scaled</span> <span class="o">=</span> <span class="n">ss</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">train_input</span><span class="p">)</span>
<span class="n">test_scaled</span> <span class="o">=</span> <span class="n">ss</span><span class="p">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test_input</span><span class="p">)</span>
</code></pre></div></div>

<p>이제 사이킷런에서 확률적 경사 하강법을 제공하는 분류 클래스 SGDClassifier를 사용하자. SGDClassifier 객체 생성시, 2개의 매개변수를 저장한다. loss는 손실 함수의 종류를 지정한다. loss는 ‘log’로 지정해보자. max_iter는 에포크 횟수를 의미한다. 10으로 지정하자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDClassifier</span>

<span class="n">sc</span> <span class="o">=</span> <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'log'</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">sc</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">sc</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">sc</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(결과) 0.773109243697479
       0.775
       c:\users\lg\appdata\local\programs\python\python37\lib\site-packages\sklearn\linear_model\_stochastic_gradient.py:577: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.
       ConvergenceWarning)
</code></pre></div></div>

<p>친절하게도, 사이킷런이 모델이 충분히 수렴하지 않았다는 ConvergenceWarning 경고를 보내줬다. SGDClassifier 객체를 다시 만들지 않고, 이미 훈련시킨 모델 sc를 추가로 더 훈련해보자. 모델을 이어서 훈련할 때는 <code class="language-plaintext highlighter-rouge">partial_fit()</code> 메소드를 사용한다. fit 메소드와 사용법은 같지만, 호출할 때마다 1에포크씩 이어서 훈련할 수 있다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sc</span><span class="p">.</span><span class="n">partial_fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">sc</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">sc</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(결과) 0.8151260504201681
       0.825
</code></pre></div></div>

<p>아직 점수가 낮지만, 에포크를 추가로 수행하니, 정확도가 향상되었다. 얼마나 더 훈련시켜야 만족할만한 정확도를 얻을 수 있을까? 어떤 기준이 있으면 좋을 것 같은데?</p>

<p>참고로 SGDClassifier는 객체에 한 번에 훈련 세트 전체를 전달했지만, 훈련 세트에서 1개씩 샘플을 꺼내어 경사 하강법 단계를 수행한다. SGDClassifier는 미니배치 경사 하강법이나, 배치 경사 하강법을 제공하지 않는다.</p>

<h2 id="8-6-에포크와-과대과소적합">8-6. 에포크와 과대/과소적합</h2>
<p>확률적 경사 하강법을 사용한 모델은 에포크 횟수에 따라서 과소적합이나 과대적합이 될 수 있다. 에포크 횟수가 적으면, 모델이 훈련 세트를 덜 학습한다. 반대로 에포크 횟수가 충분히 많으면 훈련 세트를 완전히 학습할 것이다. 훈련 세트에 아무 잘 맞는 모델이 만들어지는 것이다. 즉, 적은 에포크 횟수 동안 훈련한 모델은 과소적합된 모델일 가능성이 높다. 반면 많은 에포크 횟수 동안 훈련한 모델은 훈련 세트에 너무 잘 맞아 과대적합된 모델일 가능성이 높다. 훈련 세트 점수는 에포크가 진행될 수록 계속 증가하지만, 테스트 세트 점수는 어느 순간 감소하기 시작한다. 이 지점이 바로 과대적합되기 시작하는 곳이다. 과대적합이 시작하기 전 훈련을 멈추는 것이 바로 <strong>조기 종료(Early stopping)</strong>이다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'log'</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">train_score</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_score</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">classes</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">train_target</span><span class="p">)</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">300</span><span class="p">):</span> <span class="c1">## _는 사용하지 않고, 그냥 버리는 값을 넣어두는 용도로 사용함! 300번의 에포크
</span>    <span class="n">sc</span><span class="p">.</span><span class="n">partial_fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">classes</span><span class="o">=</span><span class="n">classes</span><span class="p">)</span>  <span class="c1"># partial_fit 메소드만 사용하려면, 해당 메소드에 전체 클래스의 레이블 전달해야함!
</span>    <span class="n">train_score</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">sc</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
    <span class="n">test_score</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">sc</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">train_score</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_score</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'epoch'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'accuracy'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p class="align-center"><img src="/assets/images/machinelearning/8-1.png" alt="그림 8-1. 코드 결과" /></p>
<p>그림 8-1. 코드 결과</p>

<p>그림을 보면, 백 번째 에포크 이후에는 훈련 세트와 테스트 세트 점수가 조금씩 벌어지는 것을 보인다. 반복 횟수를 100에 맞춰서 다시 모델을 훈련시켜보자.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sc</span> <span class="o">=</span> <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'log'</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">sc</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">sc</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">sc</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(결과) 0.957983193277311
       0.925
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">tol</code> 매개변수에서 향상될 최솟값을 지정하면, 일정 에포크 동안 성능이 최솟값 만큼 향상되지 않을 경우, 더 훈련하지 않고 자동으로 멈춘다. 여기서는 None으로 설정하여 멈추지 않고 에포크 100 횟수만큼 무조건 반복 훈련하도록 하였다. 최종 점수는 높아 보인다. 참고로 SGDRegressor은 확률적 경사 하강법을 사용한 회귀 알고리즘이다.</p>

<h2 id="8-7-힌지-손실--서포트-벡터-머신">8-7. 힌지 손실 / 서포트 벡터 머신</h2>
<p>사실 SGDClassifier 클래스의 loss 매개변수의 기본값은 ‘hinge’ 이다. <strong>힌지 손실(Hinge loss)</strong> 또는 <strong>서포트 벡터 머신(Support vector machine)</strong>라 불리는 손실 함수이다. 저자가 이에 대한 추가 설명은 하지 않았지만 서포트 벡터 머신이 널리 사용된다는 점과 SGDClassifier가 여러 종류의 손실 함수를 loss 매개변수에 지정한다는 점을 강조하였다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sc</span> <span class="o">=</span> <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'hinge'</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="n">sc</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">sc</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_scaled</span><span class="p">,</span> <span class="n">train_target</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="n">sc</span><span class="p">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_scaled</span><span class="p">,</span> <span class="n">test_target</span><span class="p">))</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(결과) 0.9495798319327731
       0.925
</code></pre></div></div>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#epoch" class="page__taxonomy-item p-category" rel="tag">epoch</a><span class="sep">, </span>
    
      <a href="/tags/#loss-function" class="page__taxonomy-item p-category" rel="tag">loss function</a><span class="sep">, </span>
    
      <a href="/tags/#machine-learning" class="page__taxonomy-item p-category" rel="tag">machine learning</a><span class="sep">, </span>
    
      <a href="/tags/#stochastic-gradient-descent" class="page__taxonomy-item p-category" rel="tag">stochastic gradient descent</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#machinelearning" class="page__taxonomy-item p-category" rel="tag">machinelearning</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2022-02-15T00:00:00+09:00">February 15, 2022</time></p>

      </footer>

      

      
  <nav class="pagination">
    
      <a href="/machinelearning/machinelearning7/" class="pagination--pager" title="[Machine learning] 7. 로지스틱 회귀
">Previous</a>
    
    
      <a href="/machinelearning/machinelearning9/" class="pagination--pager" title="[Machine learning] 9. 결정 트리
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h2 class="page__related-title">You may also enjoy</h2>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/machinelearning/machinelearning13/" rel="permalink">[Machine learning] 13. K-평균
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2022-02-23T00:00:00+09:00">February 23, 2022</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          4 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">본 포스팅은 “혼자 공부하는 머신러닝+딥러닝” 책 내용을 기반으로 작성되었습니다.
잘못된 내용이 있을 경우 지적해 주시면 감사드리겠습니다.
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/machinelearning/machinelearning12/" rel="permalink">[Machine learning] 12. 군집 알고리즘
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2022-02-22T00:00:00+09:00">February 22, 2022</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          4 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">본 포스팅은 “혼자 공부하는 머신러닝+딥러닝” 책 내용을 기반으로 작성되었습니다.
잘못된 내용이 있을 경우 지적해 주시면 감사드리겠습니다.
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/machinelearning/machinelearning11/" rel="permalink">[Machine learning] 11. 트리의 앙상블
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2022-02-19T00:00:00+09:00">February 19, 2022</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          6 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">본 포스팅은 “혼자 공부하는 머신러닝+딥러닝” 책 내용을 기반으로 작성되었습니다.
잘못된 내용이 있을 경우 지적해 주시면 감사드리겠습니다.
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/machinelearning/machinelearning10/" rel="permalink">[Machine learning] 10. 교차 검증과 그리드 서치
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2022-02-17T00:00:00+09:00">February 17, 2022</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          5 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">본 포스팅은 “혼자 공부하는 머신러닝+딥러닝” 책 내용을 기반으로 작성되었습니다.
잘못된 내용이 있을 경우 지적해 주시면 감사드리겠습니다.
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
          <li><a href="https://github.com/kiwon107" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-gitlab" aria-hidden="true"></i> GitLab</a></li>
        
      
        
      
        
          <li><a href="https://instagram.com/k1_won" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-instagram" aria-hidden="true"></i> Instagram</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2022 Kiwon Yang. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




    <script>
  'use strict';

  (function() {
    var commentContainer = document.querySelector('#utterances-comments');

    if (!commentContainer) {
      return;
    }

    var script = document.createElement('script');
    script.setAttribute('src', 'https://utteranc.es/client.js');
    script.setAttribute('repo', 'kiwon107/kiwon107.github.io');
    script.setAttribute('issue-term', 'pathname');
    
    script.setAttribute('theme', 'github-light');
    script.setAttribute('crossorigin', 'anonymous');

    commentContainer.appendChild(script);
  })();
</script>

  





  </body>
</html>
