<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>[Deeplearning(pytorch)] 16. 주어진 환경과 상호작용하여 학습하는 DQN - Wonny’s DevLog</title>
<meta name="description" content="본 포스팅은 “펭귄브로의 3분 딥러닝, 파이토치맛” 책 내용을 기반으로 작성되었습니다. 잘못된 내용이 있을 경우 지적해 주시면 감사드리겠습니다.">


  <meta name="author" content="K.W. Yang">
  
  <meta property="article:author" content="K.W. Yang">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Wonny's DevLog">
<meta property="og:title" content="[Deeplearning(pytorch)] 16. 주어진 환경과 상호작용하여 학습하는 DQN">
<meta property="og:url" content="http://localhost:4000/deeplearningpyt/deeplearningpyt16/">


  <meta property="og:description" content="본 포스팅은 “펭귄브로의 3분 딥러닝, 파이토치맛” 책 내용을 기반으로 작성되었습니다. 잘못된 내용이 있을 경우 지적해 주시면 감사드리겠습니다.">







  <meta property="article:published_time" content="2022-03-31T00:00:00+09:00">





  

  


<link rel="canonical" href="http://localhost:4000/deeplearningpyt/deeplearningpyt16/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Kiwon Yang",
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Wonny's DevLog Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

    <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    TeX: {
        equationNumbers: {
        autoNumber: "AMS"
        }
    },
    tex2jax: {
    inlineMath: [ ['$', '$'] ],
    displayMath: [ ['$$', '$$'] ],
    processEscapes: true,
    }
});
MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
        alert("Math Processing Error: "+message[1]);
    });
MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
        alert("Math Processing Error: "+message[1]);
    });
</script>
<script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/assets/images/logo.png" alt="Wonny's DevLog"></a>
        
        <a class="site-title" href="/">
          Wonny's DevLog
          <span class="site-subtitle">Version 1.0</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about/">About</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/">Category</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      



<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/images/profile.jpg" alt="K.W. Yang" itemprop="image" class="u-photo">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">K.W. Yang</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>AI Researcher &amp; Developer</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">South Korea</span>
        </li>
      

      
        
          
        
          
        
          
        
          
        
          
            <li><a href="https://github.com/kiwon107" rel="nofollow noopener noreferrer me"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
            <li><a href="https://instagram.com/k1_won" rel="nofollow noopener noreferrer me"><i class="fab fa-fw fa-instagram" aria-hidden="true"></i><span class="label">Instagram</span></a></li>
          
        
      

      

      
        <li>
          <a href="mailto:p2881p@naver.com" rel="me" class="u-email">
            <meta itemprop="email" content="p2881p@naver.com" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span>
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>
  
    
      
      
      
      
    
    
      

<nav class="nav__list">
  
  <input id="ac-toc" name="accordion-toc" type="checkbox" />
  <label for="ac-toc">Toggle menu</label>
  <ul class="nav__items">
    
      <li>
        
          <span class="nav__sub-title">AI</span>
        

        
        <ul>
          
          
            <li><a href="/machinelearning/">머신러닝(사이킷런) (14)</a></li>
          
          
            <li><a href="/deeplearningpyt/">딥러닝(파이토치) (16)</a></li>
          
          
            <li><a href="/deeplearningtens/">딥러닝(텐서플로우) (9)</a></li>
          
          
            <li><a href="/tensortextbook/">딥러닝 텐서플로 교과서 (1)</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          <span class="nav__sub-title">Language</span>
        

        
        <ul>
          
          
            <li><a href="/pythonmd/">파이썬 중급 (26)</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          <span class="nav__sub-title">CS Theory</span>
        

        
        <ul>
          
          
            <li><a href="/os/">운영체제 ()</a></li>
          
          
            <li><a href="/algopy/">자료구조와 알고리즘(파이썬) (1)</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          <span class="nav__sub-title">Mathematics</span>
        

        
        <ul>
          
          
            <li><a href="/prob/">확률과 통계 (2)</a></li>
          
        </ul>
        
      </li>
    
      <li>
        
          <a href="/paper/"><span class="nav__sub-title">논문</span></a>
        

        
      </li>
    
  </ul>
</nav>

    
  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="[Deeplearning(pytorch)] 16. 주어진 환경과 상호작용하여 학습하는 DQN">
    <meta itemprop="description" content="본 포스팅은 “펭귄브로의 3분 딥러닝, 파이토치맛” 책 내용을 기반으로 작성되었습니다.잘못된 내용이 있을 경우 지적해 주시면 감사드리겠습니다.">
    <meta itemprop="datePublished" content="2022-03-31T00:00:00+09:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title p-name" itemprop="headline">
            <a href="http://localhost:4000/deeplearningpyt/deeplearningpyt16/" class="u-url" itemprop="url">[Deeplearning(pytorch)] 16. 주어진 환경과 상호작용하여 학습하는 DQN
</a>
          </h1>
          

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2022-03-31T00:00:00+09:00">March 31, 2022</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          6 minute read
        
      </span>
    
  </p>


        </header>
      

      <section class="page__content e-content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> On this page</h4></header>
              <ul class="toc__menu"><li><a href="#16-1-강화학습과-dqn-기초">16-1. 강화학습과 DQN 기초</a></li><li><a href="#16-2-카드-게임-마스터-하기">16-2 카드 게임 마스터 하기</a></li></ul>

            </nav>
          </aside>
        
        <p>본 포스팅은 “펭귄브로의 3분 딥러닝, 파이토치맛” 책 내용을 기반으로 작성되었습니다.
잘못된 내용이 있을 경우 지적해 주시면 감사드리겠습니다.</p>

<h2 id="16-1-강화학습과-dqn-기초">16-1. 강화학습과 DQN 기초</h2>
<p><strong>강화학습(Reinforcement learning)</strong>은 주어진 환경과 상호작용하여 좋은 점수를 얻는 방향으로 성장하는 머신러닝 분야이다. 그동안 배운 학습법들은 원하는 데이터셋을 외우는 주입식 학습법이었다. 강화학습은 자기주도적 학습법이라 할 수 있다. 강화학습 모델은 주어진 환경에서 시행착오를 겪으며 좋은 피드백을 받는 쪽으로 최적화하며 성장한다.</p>

<p>강화학습은 크게 <strong>상태(State)</strong>, <strong>에이전트(Agent)</strong>, <strong>행동(Action)</strong>, <strong>보상(Reward)</strong> 4가지 요소로 나눌 수 있다.<br />
1) 에이전트: 인공지능 플레이어<br />
2) 환경: 에이전트가 솔루션을 찾기 위한 무대<br />
3) 행동: 에이전트가 환경 안에서 시행하는 상호작용<br />
4) 보상: 에이전트의 행동에 따른 점수 혹은 결과<br /></p>

<p>2013년 딥마인드는 <strong>DQN(Deep q-network)</strong> 알고리즘으로 아타리사의 유명 게임들에서 사람보다 월등히 높은 점수를 얻었다.</p>

<p class="align-center"><img src="/assets/images/deeplearningpyt/16-1.png" alt="그림 16-1. DQN 학습 구조" /></p>
<p>그림 16-1. DQN 학습 구조</p>

<h2 id="16-2-카드-게임-마스터-하기">16-2 카드 게임 마스터 하기</h2>
<p>카드폴이라는 환경을 구축하여 강화학습을 해보자. 카드폴 게임에서는 막대기를 세우고 오래 버틸수록 점수가 올라간다. 막대기가 오른쪽으로 기울 때는 중심을 다시 맞춰야 하므로 오른쪽 버튼을 눌러 검은색 상자를 오른쪽으로 움직이는 것이 왼쪽 버튼을 눌러 검은색 상자를 왼쪽으로 옮기는 것보다 보상이 클 것으로 예측할 수 있다.</p>

<p>DQN의 주요 특징은 <strong>기억하기(Memorize)</strong>와 <strong>다시보기(Replay)</strong>이다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 관련 모듈 임포트
</span><span class="kn">import</span> <span class="nn">gym</span>  <span class="c1"># 카드폴 등의 여러 게임 환경 제공하는 패키지
</span><span class="kn">import</span> <span class="nn">random</span> 
<span class="kn">import</span> <span class="nn">math</span>  
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">deque</span>  <span class="c1"># 큐(FIFO) 자료구조의 일종. 덱(Double-end queue)은 양쪽 끝에서 삽입과 삭제가 모두 가능한 자료구조
</span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</code></pre></div></div>

<p>그 다음 하이퍼파라미터를 설정한다. EPS_START와 EPS_END는 엡실론이다. 엡실론이 50%이면 절반의 확률로 무작위 행동을 하고, 나머지 절반의 확률로 학습된 방향으로 행동한다. 무작위로 행동하는 이유는 에이전트가 가능한 모든 행동을 경험하도록 하기 위해서이다. 시작값은 90%이지만 학습이 진행되면서 조금씩 감소해서 5%까지 내려가게하자.</p>

<p>감마는 에이전트가 현재 보상을 미래 보상보다 얼마나 가치 있게 여기는지에 대한 값이다. 1년 뒤 받을 만원과 지금 받는 만원의 가치는 다르므로, 1년뒤 받을 만원은 이자율 만큼 할인해줘야한다. 감마는 이 할인율과 비슷한 개념이다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 하이퍼파라미터
</span><span class="n">EPISODES</span> <span class="o">=</span> <span class="mi">50</span>  <span class="c1"># 에피소드 반복 횟수(총 플레이할 게임 수)
</span><span class="n">EPS_START</span> <span class="o">=</span> <span class="mf">0.9</span>  <span class="c1"># 학습 시작 시 에이전트가 무작위로 행동할 확률
</span><span class="n">EPS_END</span> <span class="o">=</span> <span class="mf">0.05</span>  <span class="c1"># 학습 막바지에 에이전트가 무작위로 행동할 확률
</span><span class="n">EPS_DECAY</span> <span class="o">=</span> <span class="mi">200</span>  <span class="c1"># 학습 진행 시 에이전트가 무작위로 행동할 확률을 EPS_START에서 EPS_END 까지 점진적으로 감소시키는 값
</span><span class="n">GAMMA</span> <span class="o">=</span> <span class="mf">0.8</span>  <span class="c1"># 할인계수
</span><span class="n">LR</span> <span class="o">=</span> <span class="mf">0.001</span>  <span class="c1"># 학습률
</span><span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">64</span>  <span class="c1"># 배치 크기
</span></code></pre></div></div>

<p>이제 DQN 클래스를 만들자. 에이전트의 신경망은 카트 위치, 카트 속도, 막대기 각도, 막대기 속도까지 4가지 정보를 입력받아 왼쪽으로 갈 때의 가치와 오른쪽으로 갈 때의 가치를 출력한다. 그래서 첫 번째 신경망의 입력 노드는 4개이고, 마지막 신경망의 출력 노드는 2개이다.</p>

<p class="align-center"><img src="/assets/images/deeplearningpyt/16-2.png" alt="그림 16-2. DQN 에이전트의 인공 신경망" /></p>
<p>그림 16-2. DQN 에이전트의 인공 신경망</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DQNAgent</span><span class="p">:</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
      <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span>
      <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span>
      <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">LR</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">steps_done</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div></div>

<p>딥러닝 모델들은 보통 학습 데이터 샘플이 각각 독립적이라 가정한다. 그러나 강화학습은 연속된 상태가 강한 상관관계가 있어서 학습이 어렵다는 문제점이 있다. 무작위로 가져오는 것이 아닌 연속적인 경험을 학습할 때 초반의 몇 가지 경험 패턴에만 치중하여 학습하게 된다. 그럼 최적의 행동 패턴을 찾기 어렵다. 또한 신경망이 새로운 경험을 이전 경험에 겹쳐 쓰며 쉽게 잊어버린다는 문제점도 있다.</p>

<p>그래서 기억하기 기능이 필요하다. 이전 경험들을 배열에 담아 계속 재학습시키며 신경망이 잊지 않게 하는 것이다. 기억한 경험들은 학습할 때 무작위로 뽑아 경험 간의 상관관계를 줄인다. 각 경험은 상태, 행동, 보상 등을 담아야 한다. 이번 예제에서는 사용하기 가장 간단한 큐 자료구조를 이용하여 이전 경험들에 관한 기억을 담을 것이다.
deque의 <code class="language-plaintext highlighter-rouge">mexlen</code>을 지정해주면 큐가 가득 찰 때 제일 오래된 요소부터 없어져 자연스레 오래된 기억을 잊게 해준다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="bp">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">)</span>
</code></pre></div></div>

<p class="align-center"><img src="/assets/images/deeplearningpyt/16-3.png" alt="그림 16-3. 기억을 저장하는 큐" /></p>
<p>그림 16-3. 기억을 저장하는 큐</p>

<p>이제 self.memory 배열에 새로운 경험을 덧붙일 memorize() 함수를 만들자. memorize() 함수는 self.memory 배열에 <strong>현재 상태(state)</strong>, <strong>현재 상태에서 한 행동(Action)</strong>, <strong>행동에 대한 보상(Rewrard)</strong>, <strong>행동으로 인해 새로 생성된 상태(Next_state)</strong>를 한 세트로 저장한다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">def</span> <span class="nf">memorize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">):</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="n">append</span><span class="p">((</span><span class="n">state</span><span class="p">,</span>
                        <span class="n">action</span><span class="p">,</span>
                        <span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="n">reward</span><span class="p">]),</span>
                        <span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="n">next_state</span><span class="p">])))</span>
</code></pre></div></div>

<p>이제 행동을 담당하는 act() 함수를 만들자. 무작위로 숫자를 골라 앱실론 값보다 높으면 신경망이 학습하여 이렇게 행동하는게 옳다고 생각하는 쪽으로, 낮으면 무작위로 행동한다. 학습 초반에는 학습이 덜되어, 에이전트가 하는 행동에 의미를 부여하기 어렵다. 그러므로 초반에는 엡실론값을 높게 주어 최대한 다양한 경험을 해보도록 하고, 점점 그 값을 낮춰가며 신경망이 결정하는 비율을 높힌다. 이 알고리즘을 <strong>엡실론 그리디(엡실론 탐욕, Epsilon-greedy) 알고리즘</strong>이라고 한다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
    <span class="n">eps_threshold</span> <span class="o">=</span> <span class="n">EPS_END</span> <span class="o">+</span> <span class="p">(</span><span class="n">EPS_START</span> <span class="o">-</span> <span class="n">EPS_END</span><span class="p">)</span> <span class="o">*</span> <span class="n">math</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="bp">self</span><span class="p">.</span><span class="n">steps_done</span> <span class="o">/</span> <span class="n">EPS_DECAY</span><span class="p">)</span>  <span class="c1"># 처음에는 무작위로 행동할 확률이 높음. 그러다 점점 낮아짐.
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">steps_done</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">if</span> <span class="n">random</span><span class="p">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">eps_threshold</span><span class="p">:</span>
      <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">(</span><span class="n">state</span><span class="p">).</span><span class="n">data</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">1</span><span class="p">].</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c1"># 신경망이 결정한 행동
</span>    <span class="k">else</span><span class="p">:</span>
      <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">LongTensor</span><span class="p">([[</span><span class="n">random</span><span class="p">.</span><span class="n">randrange</span><span class="p">(</span><span class="mi">2</span><span class="p">)]])</span> <span class="c1"># 무작위 행동
</span></code></pre></div></div>

<p>이전 경험들을 모아놨으면 반복적으로 학습해야 한다. <strong>경험 리플레이(Experience replay)</strong>라고 하는데, 사람이 수면 중일 때 자동차 운전, 농구 슈팅 등 운동 관련 정보를 정리하여 단기 기억을 장기 기억으로 전환하는 것과 비슷하다 보면 된다.</p>

<p>learn() 함수는 에이전트가 경험 리플레이를 하며 학습하는 역할을 수행한다. 이 함수는 방금 만든 에이전트의 신경망(self.model)을 기억(self.memory)에 쌓인 경험을 토대로 학습시킨다. self.memory에 저장된 경험들의 수가 배치 크기보다 커지기 전까진 return으로 학습을 거르고, 경험이 충분히 쌓일 때 self.memory 큐에서 무작위로 배치 크기만큼의 무작위로 가져온다. 경험들을 무작위로 가져오면서 각 경험 샘플의 상관성을 줄일 수 있다. zip(*batch)는 (state, action, reward, next_state)가 한 세트로 모여진 하나의 배열을 state, action, reward, next_state 4개의 배열로 정리해준다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">memory</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">BATCH_SIZE</span><span class="p">:</span>
      <span class="k">return</span>
    <span class="n">batch</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="n">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">memory</span><span class="p">,</span> <span class="n">BATCH_SIZE</span><span class="p">)</span>
    <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">next_states</span> <span class="o">=</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">batch</span><span class="p">)</span>
</code></pre></div></div>

<p>각각의 경험은 상태(states), 행동(actions), 행동에 따른 보상(rewards), 그다음 상태(next_states)를 담고 있다. 모두 리스트 형태이므로 torch.cat() 함수를 이용하여 하나의 텐서로 만든다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">(</span><span class="n">states</span><span class="p">)</span>
    <span class="n">actions</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
    <span class="n">next_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">(</span><span class="n">next_states</span><span class="p">)</span>
</code></pre></div></div>

<p>이제 에이전트의 신경망을 훈련시키자! 에이전트가 점점 발전된 행동을 하려면 가지고 있는 신경망이 주어진 상태에서 <strong>각 행동의 가치(Q)</strong>를 잘 예측하도록 학습돼야 한다. 그러므로, 현재 상테에서 에이전트가 생각하는 행동의 가치를 추출하는 작업이 필요하다!</p>

<p>현재 상태를 신경망에 통과시켜 왼쪽 또는 오른쪽으로 가는 행동에 대한 가치를 계산한다. gather() 함수로 에이전트가 현 상태에서 했던 행동의 가치들을 current_q에 담는다.</p>

<p>DQN 알고리즘 학습은 <strong>할인된 미래 가치</strong>로 누적된 보상을 극대화하는 방향으로 이루어진다. 미래 가치는 에이전트가 미래에 받을 수 있는 보상의 기대값이다. max() 함수로 다음 상테에서 에이전트가 생각하는 행동의 최대 가치를 구하여 max_next_q에 담는다. 할인은 현재 1의 보상을 받을 수 있다고 해도, 미래에도 1의 보상을 받을 때 현재의 보상을 좀 더 가치있게 보는 것을 말한다. GAMMA값을 곱해주어 에이전트가 보는 행동들의 미래 가치(max_next_q)를 20% 할인한다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="n">current_q</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">(</span><span class="n">states</span><span class="p">).</span><span class="n">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span>  <span class="c1"># 현재 상테에서 에이전트가 생각하는 행동의 가치 추출. 1 번째 차원에서 actions이 가리키는 값들을 추출한다!
</span>    <span class="n">max_next_q</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">model</span><span class="p">(</span><span class="n">next_states</span><span class="p">).</span><span class="n">detach</span><span class="p">().</span><span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># 에이전트가 본 행동들의 미래 가치
</span>    <span class="n">expected_q</span> <span class="o">=</span> <span class="n">rewards</span> <span class="o">+</span> <span class="p">(</span><span class="n">GAMMA</span> <span class="o">*</span> <span class="n">max_next_q</span><span class="p">)</span> <span class="c1"># 할인된 미래 가치
</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">current_q</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">expected_q</span><span class="p">)</span>  <span class="c1"># 현재 에이전트가 생각하는 행동 가치가 할인된 미래 가치를 따라가도록 학습 진행
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<p class="align-center"><img src="/assets/images/deeplearningpyt/16-4.png" alt="그림 16-4. gather() 함수로 current_q가 만들어지는 과정" /></p>
<p>그림 16-4. gather() 함수로 current_q가 만들어지는 과정</p>

<p>gym으로 게임 환경을 생성하려면 <code class="language-plaintext highlighter-rouge">make()</code> 함수에 원하는 게임 이름을 넣어주면 된다. 아래 코드에서 env 변수는 이제 게임 환경이 된다. 여기에 에이전트의 행동을 입력하여 행동에 따른 다음 상태와 게임 종료 여부를 출력한다.</p>

<p>이제 학습을 실행해보자. DQNAgent를 소환하여 agent로 인스턴스화 하자.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 학습 준비하기
</span><span class="n">agent</span> <span class="o">=</span> <span class="n">DQNAgent</span><span class="p">()</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="p">.</span><span class="n">make</span><span class="p">(</span><span class="s">'CartPole-v0'</span><span class="p">)</span>
<span class="n">score_history</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># 학습 진행 기록하기 위해 점수 저장
</span>
<span class="c1"># 학습 시작
</span><span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">EPISODES</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
  <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">reset</span><span class="p">()</span>  <span class="c1"># 게임 시작 시 초기화된 상태를 불러오는 함수
</span>  <span class="n">steps</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre></div></div>

<p>게임이 끝날 때 까지 에이전트가 행동하는 것을 멈추지 않는다. 게임 진행될 때마다 env.render() 함수로 게임 화면을 띄우자. 매 턴마다 다음과 같은 상태가 진행된다.<br />
1) 현재 게임 상태 state를 텐서로 만들고 에이전트의 행동 함수 act()의 입력으로 사용<br />
2) 상태를 받은 에이전트는 엡실론 그리디 알고리즘에 따라 행동 action을 내뱉음<br />
3) action 변수는 파이토치 텐서이므로 item() 함수로 에이전트가 한 행동의 번호를 추출하여 step() 함수에 입력해 넣어주면 에이전트의 행동에 따른 다음 상태(next_state), 보상(reward), 종료 여부(done)를 출력함<br /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="k">while</span> <span class="bp">True</span><span class="p">:</span>
    <span class="n">env</span><span class="p">.</span><span class="n">render</span><span class="p">()</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="n">state</span><span class="p">])</span>
    <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="n">act</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
    <span class="n">next_state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">.</span><span class="n">item</span><span class="p">())</span>  <span class="c1"># reward는 이상 없으면 보상 1을 줌
</span></code></pre></div></div>

<p>우리의 에이전트가 한 행동의 결과가 나왔는데 막대가 넘어져 게임이 끝났을 경우에는 -1의 처벌을 주고 이 경험을 기억하고 결과를 배우도록 한다. 학습이 진행될수록 폴대가 넘어지지 않아 게임이 끝나지 않는 방향으로 학습이 진행된다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="c1"># 게임이 끝났을 경우 마이너스 보상주기
</span>    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
      <span class="n">reward</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

    <span class="n">agent</span><span class="p">.</span><span class="n">memorize</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">)</span>
    <span class="n">agent</span><span class="p">.</span><span class="n">learn</span><span class="p">()</span>

    <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
    <span class="n">steps</span> <span class="o">+=</span> <span class="mi">1</span>
</code></pre></div></div>

<p>게임이 끝나면 done이 True가 되어 다음 코드가 실행된다. 여기서는 다음 코드로 에피소드 숫자와 점수만 표기해보자. 그리고 score_history 리스트에 점수를 담아 그래프로 나타내보자.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
      <span class="k">print</span><span class="p">(</span><span class="s">"에피소드:{0} 점수: {1}"</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">steps</span><span class="p">))</span>
      <span class="n">score_history</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">steps</span><span class="p">)</span>
      <span class="k">break</span>

<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">score_history</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'score'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p class="align-center"><img src="/assets/images/deeplearningpyt/16-5.png" alt="그림 16-5. DQN 에이전트의 점수 기록" /></p>
<p>그림 16-5. DQN 에이전트의 점수 기록</p>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#deep-learning" class="page__taxonomy-item p-category" rel="tag">deep learning</a><span class="sep">, </span>
    
      <a href="/tags/#dqn" class="page__taxonomy-item p-category" rel="tag">dqn</a><span class="sep">, </span>
    
      <a href="/tags/#pytorch" class="page__taxonomy-item p-category" rel="tag">pytorch</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#deeplearningpyt" class="page__taxonomy-item p-category" rel="tag">deeplearningpyt</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2022-03-31T00:00:00+09:00">March 31, 2022</time></p>

      </footer>

      

      
  <nav class="pagination">
    
      <a href="/deeplearningpyt/deeplearningpyt15/" class="pagination--pager" title="[Deeplearning(pytorch)] 15. cGAN으로 생성 제어하기
">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h2 class="page__related-title">You may also enjoy</h2>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deeplearningpyt/deeplearningpyt15/" rel="permalink">[Deeplearning(pytorch)] 15. cGAN으로 생성 제어하기
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2022-03-29T00:00:00+09:00">March 29, 2022</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          3 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">본 포스팅은 “펭귄브로의 3분 딥러닝, 파이토치맛” 책 내용을 기반으로 작성되었습니다.
잘못된 내용이 있을 경우 지적해 주시면 감사드리겠습니다.
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deeplearningpyt/deeplearningpyt14/" rel="permalink">[Deeplearning(pytorch)] 14. 경쟁하며 학습하는 GAN
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2022-03-28T00:00:00+09:00">March 28, 2022</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          4 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">본 포스팅은 “펭귄브로의 3분 딥러닝, 파이토치맛” 책 내용을 기반으로 작성되었습니다.
잘못된 내용이 있을 경우 지적해 주시면 감사드리겠습니다.
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deeplearningpyt/deeplearningpyt13/" rel="permalink">[Deeplearning(pytorch)] 13. 딥러닝을 해킹하는 적대적 공격
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2022-03-26T00:00:00+09:00">March 26, 2022</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          5 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">본 포스팅은 “펭귄브로의 3분 딥러닝, 파이토치맛” 책 내용을 기반으로 작성되었습니다.
잘못된 내용이 있을 경우 지적해 주시면 감사드리겠습니다.
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/deeplearningpyt/deeplearningpyt12/" rel="permalink">[Deeplearning(pytorch)] 12. Seq2Seq 기계 번역
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <i class="far fa-fw fa-calendar-alt" aria-hidden="true"></i>
        
        <time datetime="2022-03-21T00:00:00+09:00">March 21, 2022</time>
      </span>
    

    <span class="page__meta-sep"></span>

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-fw fa-clock" aria-hidden="true"></i>
        
          3 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">본 포스팅은 “펭귄브로의 3분 딥러닝, 파이토치맛” 책 내용을 기반으로 작성되었습니다.
잘못된 내용이 있을 경우 지적해 주시면 감사드리겠습니다.
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
          <li><a href="https://github.com/kiwon107" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-gitlab" aria-hidden="true"></i> GitLab</a></li>
        
      
        
      
        
          <li><a href="https://instagram.com/k1_won" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-instagram" aria-hidden="true"></i> Instagram</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2022 Kiwon Yang. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




    <script>
  'use strict';

  (function() {
    var commentContainer = document.querySelector('#utterances-comments');

    if (!commentContainer) {
      return;
    }

    var script = document.createElement('script');
    script.setAttribute('src', 'https://utteranc.es/client.js');
    script.setAttribute('repo', 'kiwon107/kiwon107.github.io');
    script.setAttribute('issue-term', 'pathname');
    
    script.setAttribute('theme', 'github-light');
    script.setAttribute('crossorigin', 'anonymous');

    commentContainer.appendChild(script);
  })();
</script>

  





  </body>
</html>
