I"<p>본 포스팅은 “펭귄브로의 3분 딥러닝, 파이토치맛” 책 내용을 기반으로 작성되었습니다.
잘못된 내용이 있을 경우 지적해 주시면 감사드리겠습니다.</p>

<h2 id="13-1-적대적-공격이란">13-1. 적대적 공격이란?</h2>
<p>머신러닝 모델의 착시를 유도하는 입력을 <strong>적대적 예제(Adversarial example)</strong>라고 한다. 적대적 예제를 생성해서 여러 가지 머신러닝 기반 시스템의 성능을 의도적으로 떨어뜨려 보안 문제를 일으키는 것을 <strong>적대적 공격(Adversarial attack)</strong>라고 한다.<br /></p>

<p>최근 구글 브레인에서 진행한 연구에 따르면, 모든 머신러닝 분류기가 속임수에 의해 잘못된 예측을 할 수 있다고 한다. 자율주행, 은행의 비정상거래탐지, 의료영상분석 등 실수가 용납되지 않는 분야의 시스템에 신뢰도를 떨어뜨리는 약점이 존재할 수 있다는 것은 치명적이다. 딥러닝 모델 내부를 해석할 수 있다면 이러한 적대적 공격에 대해 보안 기능을 마련할 수 있을 것이다. 그러나 아직 이러한 기술은 초기 단계에 있어 최근 연구에서도 효과적인 방어법을 내놓지 못하고 있다.<br /></p>

<p>입력 데이터가 신경망 모델을 타고 흐르면서 계속 변환이 일어난다. 각 변환은 입력의 특정 구조에 매우 예민하게 반응한다. 이처럼 모델이 예민하게 반응하는 부분을 공략하여 모델을 헷갈리게 할 수 있다.<br /></p>

<h2 id="13-2-적대적-공격의-정류">13-2. 적대적 공격의 정류</h2>
<p>적대적 공격은 적절한 잡음을 생성하여 사람의 눈에는 똑같이 보이지만 머신러닝 모델을 헷갈리게 만드는 적대적 예제를 생성하는 것이 핵심이다. 인식 오류를 일으키지만 원본과 차이가 가장 적은 잡음을 찾는 것이고, 결국 최적화 문제로 해석할 수 있다. 적대적 공격에선 오차를 줄이기보다는 극대화하는 쪽으로 잡음을 최적화한다.<br /></p>

<p>잡음을 생성하는 방법은 많다. 다만 모델 정보가 필요한지, 우리가 원하는 정답으로 유도할 수 있는지, 여러 모델을 동시에 헷갈리게 할 수 있는지, 학습이 필요한지 등의 여부에 따라 종류가 나뉜다. 극단적으로 이미지 픽셀 하나만 건드려 분류기의 예측을 완전히 빗나가게 할 수도 있다.<br /></p>

<p>적대적 예제에서 잡음의 생성 방법은 분류 기준이 무엇이냐에 따라 여러가지로 나뉜다.<br />
1) 기울기와 같은 모델 정보가 필요한지에 따라, 모델 정보를 토대로 잡음을 생성하는 화이트박스 방법과 모델 정보 없이 생성하는 블랙박스로 나뉜다.<br />
2) 원하는 정답으로 유도할 수 있다면 표적, 아니라면 비표적으로 분류한다.<br />
3) 잡음을 생성하기 위해 반복된 학습(최적화)이 필요하면 반복, 아니면 원샷으로 나눌 수 있다.<br /></p>
:ET