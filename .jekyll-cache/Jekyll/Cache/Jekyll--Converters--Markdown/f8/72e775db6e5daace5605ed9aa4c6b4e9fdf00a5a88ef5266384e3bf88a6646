I"†Z<p>ë³¸ í¬ìŠ¤íŒ…ì€ â€œí­ê·„ë¸Œë¡œì˜ 3ë¶„ ë”¥ëŸ¬ë‹, íŒŒì´í† ì¹˜ë§›â€ ì±… ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
ì˜ëª»ëœ ë‚´ìš©ì´ ìˆì„ ê²½ìš° ì§€ì í•´ ì£¼ì‹œë©´ ê°ì‚¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤.</p>

<h2 id="6-1-ê³¼ëŒ€ì í•©-ê³¼ì†Œì í•©-ì¡°ê¸°ì¢…ë£Œ">6-1. ê³¼ëŒ€ì í•©, ê³¼ì†Œì í•©, ì¡°ê¸°ì¢…ë£Œ</h2>
<p>ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ë§Œë“¤ë©´ í•™ìŠµ ì„±ëŠ¥ì€ ì˜ ë‚˜ì˜¤ì§€ë§Œ, í…ŒìŠ¤íŠ¸ì…‹ì´ë‚˜ ì‹¤ì œ ìƒí™©ì—ì„œëŠ” ì„±ëŠ¥ì´ ë‚˜ì˜¤ì§€ ì•Šì„ ë•Œê°€ ìˆë‹¤. ì´ê²ƒì„ <strong>ê³¼ëŒ€ì í•©(Overfitting)</strong> ì´ë¼ê³  í•œë‹¤. ì¦‰, ë„ˆë¬´ í•™ìŠµ ë°ì´í„°ì—ë§Œ ì¹˜ì¤‘ë˜ì–´ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ì„œëŠ” ì„±ëŠ¥ì´ ì˜ ë‚˜ì˜¤ì§€ ì•ŠëŠ” ìƒí™©ì„ ë§í•œë‹¤. ë°˜ëŒ€ë¡œ, í•™ìŠµì„ ì œëŒ€ë¡œ ì§„í–‰í•˜ì§€ ì•Šì€ ìƒí™©ì„ <strong>ê³¼ì†Œì í•©(Underfitting)</strong> ì´ë¼ê³  í•œë‹¤. ì´ ê²½ìš°ëŠ” í•™ìŠµ ë°ì´í„°ë„ ì œëŒ€ë¡œ í•™ìŠµí•˜ì§€ ì•ŠëŠ” ê²½ìš°ì´ë‹¤.<br />
ê°€ì¥ ë² ìŠ¤íŠ¸ì¸ ìƒíƒœëŠ”, ê³¼ì†Œì í•©ê³¼ ê³¼ëŒ€ì í•©ì˜ ì¤‘ê°„ì´ë‹¤. í•™ìŠµ ë°ì´í„°ì™€ í•™ìŠµí•˜ì§€ ì•Šì€ ì‹¤ì œ ë°ì´í„°ì—ì„œ ë™ì‹œì— ë†’ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” ìƒíƒœê°€ ë°”ë¡œ <strong>ì¼ë°˜í™”(Generalization)</strong> ë¼ê³  í•œë‹¤.<br /></p>

<p>5ì¥ì—ì„œ, ë¨¸ì‹ ëŸ¬ë‹ì—ì„œëŠ” ë³´í†µ ë°ì´í„°ì…‹ì„ í•™ìŠµ, ê²€ì¦, í…ŒìŠ¤íŠ¸ì…‹ìœ¼ë¡œ ë‚˜ëˆˆë‹¤ê³  í•˜ì˜€ë‹¤. ì´ëŠ” ê³¼ëŒ€ì í•©, ê³¼ì†Œì í•©ì„ íƒì§€í•˜ë ¤ëŠ” ë…¸ë ¥ì˜ ì¼í™˜ì´ë‹¤. í•™ìŠµ ë°ì´í„°ì…‹ìœ¼ë¡œ ê³„ì† í•™ìŠµí•˜ë©´ ì˜¤ì°¨ëŠ” ë¬´í•œì • ë‚´ë ¤ê°€ê²Œ ëœë‹¤. ê·¸ëŸ¬ë‹¤ ë³´ë©´, í•™ìŠµ ì„±ëŠ¥ì€ ê³„ì† ì¢‹ì•„ì§€ì§€ë§Œ, ê²€ì¦/í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ì€ ì˜¤íˆë ¤ ë–¨ì´ì§€ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ! í•™ìŠµ ì¤‘ê°„ì¤‘ê°„ ê²€ì¦ìš© ë°ì´í„°ì…‹ìœ¼ë¡œ ëª¨ë¸ì´ í•™ìŠµ ë°ì´í„°ì—ë§Œ ê³¼ëŒ€ì í•©ë˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸ì´ í•„ìš”í•˜ë‹¤!<br />
ê²€ì¦ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì„±ëŠ¥ì´ ë‚˜ë¹ ì§€ê¸° ì‹œì‘í•˜ë©´, ì§ì „ì´ ê°€ì¥ ì í•©í•œ ëª¨ë¸ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ì´ íƒ€ì´ë°ì— ëª¨ë¸ì„ ì €ì¥í•´ì„œ ì´ìš©í•˜ëŠ” ê²ƒì„ ì¡°ê¸° ì¢…ë£Œ(Early stopping) ì´ë¼ê³  í•œë‹¤. ì¦‰, ê²€ì¦ ì˜¤ì°¨ê°€ ì˜¬ë¼ê°€ëŠ” ìˆœê°„ì„ í¬ì°©í•´ì„œ í•™ìŠµì„ ì¢…ë£Œí•˜ëŠ” ê²ƒì´ë‹¤.</p>

<p>ê³¼ëŒ€ì í•©ì„ ë§‰ëŠ” ë°©ë²•ì€ <strong>í•™ìŠµ ë°ì´í„°ë¥¼ ëŠ˜ë¦¬ëŠ” ë°©ë²•</strong>ê³¼ <strong>ë“œë¡­ì•„ì›ƒì„ ì ìš©í•˜ëŠ” ë°©ë²•</strong>ì´ ìˆë‹¤.</p>

<h2 id="6-2-ë°ì´í„°-ëŠ˜ë¦¬ê¸°">6-2. ë°ì´í„° ëŠ˜ë¦¬ê¸°</h2>
<p>ê¶ê·¹ì ìœ¼ë¡œ, ì„¸ìƒì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ëª¨ìœ¼ëŠ” ê²ƒì´ ê°€ì¥ íš¨ê³¼ê°€ ì¢‹ë‹¤(ëª¨ìœ¼ê¸°ê°€ í˜ë“¤ë¿â€¦). ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì–»ê¸°ê°€ ì–´ë µë‹¤ë©´, ì´ë¯¸ ê°€ì§„ ë°ì´í„°ë¥¼ ìµœëŒ€í•œ ëŠ˜ë¦¬ëŠ” ë°©ë²•(Data Augmentaion)ì„ ì°¾ì•„ì•¼ í•œë‹¤. ì´ë¯¸ì§€ ë°ì´í„°ë¼ë©´ ì´ë¯¸ì§€ ì¼ë¶€ë¥¼ ìë¥´ê±°ë‚˜, ëŒë¦¬ê±°ë‚˜, ë…¸ì´ì¦ˆë¥¼ ë”í•˜ê±°ë‚˜, ìƒ‰ìƒì„ ë³€ê²½í•˜ëŠ” ë“± ì—¬ëŸ¬ê°€ì§€ ë°©ë²•ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ì˜ˆì œì—ì„œëŠ” ê°€ë¡œ ëŒ€ì¹­ì´ë™(ì™¼ìª½, ì˜¤ë¥¸ìª½ ë’¤ì§‘ê¸°) ì „ëµì„ ì¨ë¨¹ì–´ ë³´ì. í† ì¹˜ë¹„ì „ì˜ <code class="language-plaintext highlighter-rouge">transform</code> íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•˜ë©´ ê°„ë‹¨í•˜ë‹¤.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span><span class="p">,</span> <span class="n">datasets</span>

<span class="n">USE_CUDA</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span>
<span class="n">DEVICE</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda"</span> <span class="k">if</span> <span class="n">USE_CUDA</span> <span class="k">else</span> <span class="s">"cpu"</span><span class="p">)</span>

<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">64</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">(</span>
  <span class="n">datasets</span><span class="p">.</span><span class="n">MNIST</span><span class="p">(</span><span class="s">'./.data'</span><span class="p">,</span>
                  <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                  <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                  <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span>
                    <span class="n">transforms</span><span class="p">.</span><span class="n">RandomHorizontalFlip</span><span class="p">(),</span> <span class="c1"># ì´ë¯¸ì§€ë¥¼ ë¬´ì‘ìœ„ë¡œ ìˆ˜í‰ ë’¤ì§‘ê¸° ìˆ˜í–‰! í•™ìŠµ ë°ì´í„°ì…‹ì—ë§Œ ì ìš©!
</span>                    <span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                    <span class="n">transforms</span><span class="p">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))</span>
                  <span class="p">])),</span>
  <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">(</span>
  <span class="n">datasets</span><span class="p">.</span><span class="n">MNIST</span><span class="p">(</span><span class="s">'./.data'</span><span class="p">,</span>
                  <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                  <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                  <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span>
                    <span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                    <span class="n">transforms</span><span class="p">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))</span>
                  <span class="p">])),</span>
  <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>
<p>ì´ë¯¸ì§€ë¥¼ ë’¤ì§‘ëŠ” ê²ƒë§Œìœ¼ë¡œë„ í•™ìŠµ ë°ì´í„° ìˆ˜ê°€ 2ë°° ëŠ˜ì—ˆë‹¤!</p>

<h2 id="6-3-ë“œë¡­ì•„ì›ƒ">6-3. ë“œë¡­ì•„ì›ƒ</h2>
<p>ë“œë¡­ì•„ì›ƒì€ í•™ìŠµ ì§„í–‰ ê³¼ì •ì—ì„œ ì‹ ê²½ë§ì˜ ì¼ë¶€ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ë°©ë²•ì´ë‹¤. ë§Œì•½ 50% ë“œë¡­ì•„ì›ƒì„ í•œë‹¤ë©´, í•™ìŠµ ë‹¨ê³„ë§ˆë‹¤ ì ˆë°˜ì˜ ë‰´ëŸ°ë§Œ ì‚¬ìš©í•˜ë„ë¡ í•œë‹¤. ê·¸ë¦¬ê³  ê²€ì¦ê³¼ í…ŒìŠ¤íŠ¸ ë‹¨ê³„ì—ì„œëŠ” ëª¨ë“  ë‰´ëŸ°ì„ ì‚¬ìš©í•œë‹¤. í•™ìŠµì—ì„œ ë°°ì¬ëœ ë‰´ëŸ° ì™¸ì— ë‹¤ë¥¸ ë‰´ëŸ°ë“¤ì— ê°€ì¤‘ì¹˜ë¥¼ ë¶„ì‚°ì‹œí‚¤ê³ , ê°œë³„ ë‰´ëŸ°ì´ íŠ¹ì§•ì— ê³ ì •ë˜ëŠ” í˜„ìƒì„ ë°©ì§€í•˜ëŠ” ê¸°ëŠ¥ì„ í•œë‹¤.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">dropout_p</span> <span class="o">=</span> <span class="n">dropout_p</span>  <span class="c1"># ë“œë¡­ì•„ì›ƒ í™•ë¥  ê¸°ë³¸ê°’ 0.2ë¡œ ì„¤ì •! í•™ìŠµì‹œ 20% ë‰´ëŸ°ì„ ì‚¬ìš©í•˜ì§€ ì•Šê² ë‹¤ëŠ” ì˜ë¯¸
</span>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="c1"># ê°€ì¤‘ì¹˜ê°€ ì—†ìœ¼ë¯€ë¡œ, torch.nn.functional íŒ¨í‚¤ì§€ì—ì„œ ê°€ì ¸ì™€ì„œ ì‚¬ìš©! nn.Dropout í´ë˜ìŠ¤ ì‚¬ìš©ë„ ê°€ëŠ¥!
</span>    <span class="c1"># self.trainingì€ ëª‡ ê°€ì§€ ë‚´ë¶€ ë³€ìˆ˜ë¥¼ ìë™ìœ¼ë¡œ ì ìš©í•´ì£¼ëŠ” ëª¨ë“ˆì´ë‹¤.
</span>    <span class="c1"># model.train()ê³¼ model.eval()ë¡œ í•™ìŠµëª¨ë“œëƒ, í‰ê°€ëª¨ë“œëƒì— ë”°ë¼ ëª¨ë¸ ë‚´ë¶€ì˜ self.training ë³€ìˆ˜ê°’ì´ True ë˜ëŠ” Falseë¡œ ë°”ë€ë‹¤!
</span>    <span class="c1"># fc1 ì§€ë‚˜ë©´ì„œ 1ë²ˆ ìˆ˜í–‰
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout_p</span><span class="p">)</span> 
    
    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="c1"># fc2 ì§€ë‚˜ë©´ì„œ ë˜ ë‹¤ì‹œ 1ë²ˆ ìˆ˜í–‰
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="p">.</span><span class="n">dropout_p</span><span class="p">)</span>

    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="n">dropout_p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
</code></pre></div></div>

<p>ì´ì œ ë‚˜ë¨¸ì§€ ì½”ë“œë„ ì‘ì„±í•´ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•´ë³´ì!</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
  <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>

  <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
      <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">),</span> <span class="n">target</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
      <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
      <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
      <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
      <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
      <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">):</span>
  <span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
  <span class="n">test_loss</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
      <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">),</span> <span class="n">target</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
      <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
      <span class="n">test_loss</span> <span class="o">+=</span> <span class="n">F</span><span class="p">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'sum'</span><span class="p">).</span><span class="n">item</span><span class="p">()</span>
      <span class="n">pred</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
      <span class="n">correct</span> <span class="o">+=</span> <span class="n">pred</span><span class="p">.</span><span class="n">eq</span><span class="p">(</span><span class="n">target</span><span class="p">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">pred</span><span class="p">)).</span><span class="nb">sum</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
  
  <span class="n">test_loss</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">)</span>
  <span class="n">test_accuracy</span> <span class="o">=</span> <span class="mf">100.</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_accuracy</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">EPOCHS</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
  <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
  <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">)</span>
  <span class="k">print</span><span class="p">(</span><span class="s">'[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_accuracy</span><span class="p">))</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(ê²°ê³¼) [1] Test Loss: 0.5419, Accuracy: 83.04%
       [2] Test Loss: 0.4139, Accuracy: 87.15%
       [3] Test Loss: 0.3397, Accuracy: 89.55%
       [4] Test Loss: 0.2877, Accuracy: 91.15%
       [5] Test Loss: 0.2485, Accuracy: 92.31%
       [6] Test Loss: 0.2207, Accuracy: 93.18%
       [7] Test Loss: 0.2026, Accuracy: 93.71%
       [8] Test Loss: 0.1851, Accuracy: 94.25%
       [9] Test Loss: 0.1743, Accuracy: 94.55%
       [10] Test Loss: 0.1636, Accuracy: 94.83%
       [11] Test Loss: 0.1539, Accuracy: 95.21%
       [12] Test Loss: 0.1481, Accuracy: 95.39%
       [13] Test Loss: 0.1409, Accuracy: 95.57%
       [14] Test Loss: 0.1377, Accuracy: 95.70%
       [15] Test Loss: 0.1325, Accuracy: 95.87%
       [16] Test Loss: 0.1317, Accuracy: 95.87%
       [17] Test Loss: 0.1267, Accuracy: 96.01%
       [18] Test Loss: 0.1245, Accuracy: 95.95%
       [19] Test Loss: 0.1207, Accuracy: 96.07%
       [20] Test Loss: 0.1190, Accuracy: 96.16%
       [21] Test Loss: 0.1147, Accuracy: 96.37%
       [22] Test Loss: 0.1126, Accuracy: 96.32%
       [23] Test Loss: 0.1101, Accuracy: 96.44%
       [24] Test Loss: 0.1097, Accuracy: 96.44%
       [25] Test Loss: 0.1070, Accuracy: 96.41%
       [26] Test Loss: 0.1037, Accuracy: 96.63%
       [27] Test Loss: 0.1002, Accuracy: 96.73%
       [28] Test Loss: 0.1031, Accuracy: 96.72%
       [29] Test Loss: 0.1020, Accuracy: 96.69%
       [30] Test Loss: 0.0989, Accuracy: 96.85%
       [31] Test Loss: 0.0996, Accuracy: 96.81%
       [32] Test Loss: 0.0976, Accuracy: 96.88%
       [33] Test Loss: 0.0953, Accuracy: 96.92%
       [34] Test Loss: 0.0960, Accuracy: 96.96%
       [35] Test Loss: 0.0931, Accuracy: 97.04%
       [36] Test Loss: 0.0923, Accuracy: 97.08%
       [37] Test Loss: 0.0952, Accuracy: 97.08%
       [38] Test Loss: 0.0937, Accuracy: 97.14%
       [39] Test Loss: 0.0914, Accuracy: 97.20%
       [40] Test Loss: 0.0903, Accuracy: 97.15%
       [41] Test Loss: 0.0904, Accuracy: 97.22%
       [42] Test Loss: 0.0882, Accuracy: 97.15%
       [43] Test Loss: 0.0902, Accuracy: 97.01%
       [44] Test Loss: 0.0870, Accuracy: 97.16%
       [45] Test Loss: 0.0879, Accuracy: 97.16%
       [46] Test Loss: 0.0864, Accuracy: 97.26%
       [47] Test Loss: 0.0856, Accuracy: 97.36%
       [48] Test Loss: 0.0856, Accuracy: 97.26%
       [49] Test Loss: 0.0866, Accuracy: 97.23%
       [50] Test Loss: 0.0858, Accuracy: 97.29%    
</code></pre></div></div>

:ET