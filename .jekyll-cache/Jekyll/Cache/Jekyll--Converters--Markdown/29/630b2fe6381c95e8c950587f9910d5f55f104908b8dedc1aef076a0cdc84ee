I"^<p>본 포스팅은 “펭귄브로의 3분 딥러닝, 파이토치맛” 책 내용을 기반으로 작성되었습니다.
잘못된 내용이 있을 경우 지적해 주시면 감사드리겠습니다.</p>

<h2 id="7-1-컴퓨터가-보는-이미지">7-1. 컴퓨터가 보는 이미지</h2>
<p>컴퓨터에서 모든 이미지는 픽셀값들을 가로, 세로로 늘어놓은 행렬로 표현된다. 보통 인공 신경망은 다양한 형태의 입력에 대한 확정성이 떨어진다. 같은 신발 이미지라고 해도, 신발이 옆으로 조금만 치우쳐지면 예측률이 급격히 떨어진다. 특징을 추출하는 가중치가 가운데만 집중하도록 학습되었기 때문이다. 토치비전의 <code class="language-plaintext highlighter-rouge">transforms</code> 도구를 쓴다해도, 고화질 이미지의 크기를 고려하면 각 배치당 처리해야하는 데이터 수는 엄청 늘어난다. 신발의 가운데, 왼쪽 등 다양하게 배치된 이미지를 따로 학습하는 것도 이상적이지 않다. 인공지능이 하나를 배우면서 다른 하나도 자연스럽게 유추하도록 만들순 없을까?</p>

<h2 id="7-2-컨볼루션">7-2. 컨볼루션</h2>
<p>컨볼루션의 목적은 계측정으로 인식할 수 있도록 단계마다 이미지의 특징을 추출하는 것이다. 각 단계에서는 이미지에 대한 다양한 필터를 적용하여 윤곽선, 질감, 털 등 각종 특징을 추출한다. 윤곽선을 검출하는 필터를 사용하면, 밑그림을 그린것 같은 이미지가 출력된다. 이러한 필터를 적용할 때, 이미지 왼쪽 위에서 오른쪽 밑까지 밀어가며 곱하고 더하는데, 이 작업을 <strong>컨볼루션(Convolution)</strong>이라고 한다. CNN은 이미지를 추출하는 필터를 학습한다. 필터가 하나의 작은 신경망인 것이다.</p>

<h2 id="7-3-cnn-모델">7-3. CNN 모델</h2>
<p>CNN 모델은 컨볼루션 계층, 풀링 계층, 일반적인 인공 신경망 계층으로 구성된다.</p>
<ul>
  <li>컨볼루션 계층: 이미지의 특징을 추출하는 역할.</li>
  <li>풀링 계층: 필터를 거친 여러 특징 중 가장 중요한 특징 하나를 골라냄. 덜 중요한 특징을 버려 이미지의 차원 감소함.</li>
</ul>

<p>컨벌루션 연산은 이미지를 겹치는 매우 작은 조각으로 쪼개어 필터 기능을 하는 작은 신경망에 적용한다. 이 신경망은 모든 조각에 동일하게 적용되며, 특징을 추출하기 때문에 <strong>컨볼루션 필터</strong> 또는 <strong>커널</strong>이라고 부른다. 보통 3 x 3 또는 5 x 5 크기의 커널이 쓰이는데, 컨볼루션 계층 하나에 여러 개 존재할 수 있다. 학습이 시작되면, 필터 행렬의 값은 특징을 잘 뽑을 수 있도록 최적화 된다. 컨볼루션은 오른쪽 아래로 움직이며 다음 이미지를 만드는데, 한 칸 또는 여러 칸을 건너뛸 수 있다. 이 움직임을 조절하는 값을 <strong>스트라이드</strong>라고 한다. 스트라이드를 크게 주어 여러 칸을 건너뛰면 텐서의 크기는 작아지게 된다. 컨볼루션을 거쳐 만들어진 새로운 이미지는 <strong>특징 맵</strong> 이라고 부른다. 컨볼루션 계층 마다 여러 특징 맵이 만들어지고, 다음 단계인 풀링 계층으로 넘어가게 된다. 특징 맵 크기가 크면 과적합의 위험이 증가하므로, 풀링 계층에서 특징 맵의 크기를 줄여주기 위해 특징을 값 하나로 추려내어 특징을 강조하도록 한다. 보통 필터가 지나갈 때마다 픽셀을 묶어서 평균이나 최대값을 가져오는 간단한 연산이 이루어 진다. 특징 맵을 관찰하면, CNN 모델이 이미지를 계층적으로 인식한다는 것을 확인할 수 있다. 특징은 선 같은 저수준에서 눈, 코, 입 거쳐 얼굴 같은 고수준 특징이 추출되게 된다. 결과적으로, CNN은 사물이 조금만 치우쳐져도 인식하지 못하던 인공 신경망의 문제를 이미지 전체에 필터를 적용하여 특징을 추출하는 방식으로 해결해준다.</p>

<h2 id="7-4-cnn-모델-구현하기">7-4. CNN 모델 구현하기</h2>
<p>이제 코드로 구현해보자. 데이터셋을 만드는거 까지 쭉 가보자.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span><span class="p">,</span> <span class="n">datasets</span>

<span class="n">USE_CUDA</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span>
<span class="n">DEVICE</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda'</span> <span class="k">if</span> <span class="n">USE_CUDA</span> <span class="k">else</span> <span class="s">'cpu'</span><span class="p">)</span>

<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">40</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">64</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">(</span>
  <span class="n">datasets</span><span class="p">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="s">'./data'</span><span class="p">,</span>
                  <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                  <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                  <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span>
                    <span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                    <span class="n">transforms</span><span class="p">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))</span>
                  <span class="p">])),</span>
  <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">(</span>
  <span class="n">datasets</span><span class="p">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="s">'./.data'</span><span class="p">,</span>
                  <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                  <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span>
                    <span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                    <span class="n">transforms</span><span class="p">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))</span>
                  <span class="p">])),</span>
  <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>이제 CNN 모델을 설계할 차례다. <code class="language-plaintext highlighter-rouge">nn.Conv2d</code> 함수를 활용할 것이다. 이 함수의 첫 두 파라미터는 입력 채널 수 <code class="language-plaintext highlighter-rouge">in_channels</code>와 출력 채널 수 <code class="language-plaintext highlighter-rouge">out_channels</code> 이다. Fashion MNIST 데이터셋은 흑백이미지라, 색상 채널이 1개이다. 첫 컨볼루션 계층에서는 10개 특징 맵을 생성하고, 두 번째 컨볼루션 계층은 10개의 특징 맵을 받아 20개의 특징 맵을 만들도록 해보자. 커널 사이즈는 5x5로 만들 것이다. 커널 사이즈에 숫자 하나만 지정하면 정사각형으로 간주한다. (3, 5)로 입력하면 3 x 5 크기의 직사각형 커널을 만들수 도 있다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CNN</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">CNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">conv2d</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">drop</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout2d</span><span class="p">()</span> <span class="c1"># 이번에는 functional 대신 nn.Dropout2D 모듈 활용해봄!
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">320</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>  <span class="c1"># 계층의 출력 크기는 특별한 이유 없이 계층이 진행될수록 작아지도록 임의로 정함.
</span>  
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span>  <span class="c1"># F.max_pool2d() 함수의 두 번째 입력은 커널 크기! 학습 파라미터 따로 없음.
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span>  <span class="c1"># F.max_pool2d() 외, nn.MaxPool2d 같은 일반 모듈도 사용 가능.
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">320</span><span class="p">)</span>   <span class="c1"># 320은 x가 가진 원소 개수 의미
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>이제 나머지 코드를 작성하자!</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">CNN</span><span class="p">().</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>
  <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">batch_idx</span> <span class="o">%</span> <span class="mi">200</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># batch_idx는 데이터를 64개 씩 몇 번째 가져오고 있는지를 의미 
</span>      <span class="k">print</span><span class="p">(</span><span class="s">'Train Epoch: {} [{}/{} ({:.0f}%)]</span><span class="se">\t</span><span class="s">Loss:[:.6f}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">),</span> <span class="mf">100.</span> <span class="o">*</span> <span class="n">batch_idx</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">),</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()))</span>

<span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">):</span>
  <span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
  <span class="n">test_loss</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
      <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">),</span> <span class="n">target</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
      <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
      <span class="n">test_loss</span> <span class="o">+=</span> <span class="n">F</span><span class="p">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'sum'</span><span class="p">).</span><span class="n">item</span><span class="p">()</span>
      <span class="n">pred</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
      <span class="n">correct</span> <span class="o">+=</span> <span class="n">pred</span><span class="p">.</span><span class="n">eq</span><span class="p">(</span><span class="n">target</span><span class="p">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">pred</span><span class="p">)).</span><span class="nb">sum</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>

  <span class="n">test_loss</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">)</span>
  <span class="n">test_accuracy</span> <span class="o">=</span> <span class="mf">100.</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_accuracy</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">EPOCHS</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
  <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
  <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">)</span>

  <span class="k">print</span><span class="p">(</span><span class="s">'[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_accuracy</span><span class="p">))</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(결과) Train Epoch: 1 [0/60000 (0%)]	Loss:2.300701
       Train Epoch: 1 [12800/60000 (21%)]	Loss:1.112408
       Train Epoch: 1 [25600/60000 (43%)]	Loss:0.926304
       Train Epoch: 1 [38400/60000 (64%)]	Loss:0.991300
       Train Epoch: 1 [51200/60000 (85%)]	Loss:0.779528
       [1] Test Loss: 0.6457, Accuracy: 75.35%
       Train Epoch: 2 [0/60000 (0%)]	Loss:0.681881
       Train Epoch: 2 [12800/60000 (21%)]	Loss:1.025114
       Train Epoch: 2 [25600/60000 (43%)]	Loss:0.503789
       Train Epoch: 2 [38400/60000 (64%)]	Loss:0.849935
       Train Epoch: 2 [51200/60000 (85%)]	Loss:0.603705
       [2] Test Loss: 0.5394, Accuracy: 79.39%
       ...
       ...
       ...
       Train Epoch: 40 [0/60000 (0%)]	Loss:0.231346
       Train Epoch: 40 [12800/60000 (21%)]	Loss:0.226066
       Train Epoch: 40 [25600/60000 (43%)]	Loss:0.247825
       Train Epoch: 40 [38400/60000 (64%)]	Loss:0.190665
       Train Epoch: 40 [51200/60000 (85%)]	Loss:0.155920
       [40] Test Loss: 0.2999, Accuracy: 89.65%
</code></pre></div></div>
:ET