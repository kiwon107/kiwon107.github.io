I" 
<p>본 포스팅은 “혼자 공부하는 머신러닝+딥러닝” 책 내용을 기반으로 작성되었습니다.
잘못된 내용이 있을 경우 지적해 주시면 감사드리겠습니다.</p>

<h2 id="14-1-차원과-차원-축소">14-1. 차원과 차원 축소</h2>
<p>13장에서 과일 사진은 10.000개의 픽셀이 있었다. 이는 10,000개의 특성이 있는것과 같다. 이런 특성을 머신러닝 에서는 <strong>차원(Dimension)</strong>이라고 한다. 이 차원을 줄일 수 있다면 저장 공간을 크게 절약할 수 있다. 참고로 다차원 배열에서 차원은 배열의 축 개수이다. 예를 들어 2차원 배열에서는 행과 열이 차원이 된다. 그러나 1차원 배열에서는 원소의 개수가 차원이다.<br /></p>

<p>이제 비지도 학습 작업 중 하나인 <strong>차원 축소(Dimensinoality reduction)</strong>를 알아보자. 특성이 많으면 선형 모델의 성능이 높아지고 훈련 데이터에 쉽게 과적합된다. 차원 축소는 데이터를 가장 잘 나타내는 일부 특성을 선택하여 데이터의 크기를 줄이고 지도 학습 모델의 성능은 향상시킬 수 있는 방법이다. 물론 줄어든 차원에서 다시 원본 차원으로 손실을 최대한 줄이며 복원할 수도 있다.</p>

<h2 id="14-2-주성분-분석-소개">14-2. 주성분 분석 소개</h2>
<p><strong>주성분 분석(Principal component analysis)</strong>은 데이터의 분산이 큰 방향을 찾는 것으로 이해할 수 있다. 분산이란 데이터가 널리 퍼져있는 정도를 말한다. 분산이 큰 방향은 데이터로 잘 표현하는 벡터라 생각할 수 있다. 이렇게 분산이 큰 방향의 벡터를 <strong>주성분(Principal component)</strong>라고 한다. 주성분 벡터는 원본 데이터에 있는 어떤 방향이다. 주성분은 원본 차원과 같으며, 원본 데이터는 주성분을 사용하여 차원을 줄일 수 있다.<br />
주성분은 가장 분산이 큰 방향이므로 주성분에 투영하여 바꾼 데이터는 원본이 가지고 있는 특성을 가장 잘 나타낸다. 첫 번째 주성분을 찾고 이 벡터에 수직이며 분산이 가장 큰 다음 방향을 찾으면, 이 벡터가 두 번째 주성분이 된다.</p>

<h2 id="14-3-pca-클래스">14-3. PCA 클래스</h2>
<p>```python
import numpy as np
fruits = np.load(‘fruits_300.npy’)
fruits_2d = fruits.reshape(-1, 100*100)</p>

<p>from sklearn.decomposition import PCA
pca = PCA(n_components=50)
pca.fit(fruits_2d)
print(pca.components_.shape)</p>
:ET