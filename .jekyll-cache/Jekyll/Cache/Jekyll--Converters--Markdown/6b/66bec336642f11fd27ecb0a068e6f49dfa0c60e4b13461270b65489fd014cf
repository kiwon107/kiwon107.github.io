I"(1<p>본 포스팅은 “펭귄브로의 3분 딥러닝, 파이토치맛” 책 내용을 기반으로 작성되었습니다.
잘못된 내용이 있을 경우 지적해 주시면 감사드리겠습니다.</p>

<h2 id="13-1-적대적-공격이란">13-1. 적대적 공격이란?</h2>
<p>머신러닝 모델의 착시를 유도하는 입력을 <strong>적대적 예제(Adversarial example)</strong>라고 한다. 적대적 예제를 생성해서 여러 가지 머신러닝 기반 시스템의 성능을 의도적으로 떨어뜨려 보안 문제를 일으키는 것을 <strong>적대적 공격(Adversarial attack)</strong>라고 한다.<br /></p>

<p>최근 구글 브레인에서 진행한 연구에 따르면, 모든 머신러닝 분류기가 속임수에 의해 잘못된 예측을 할 수 있다고 한다. 자율주행, 은행의 비정상거래탐지, 의료영상분석 등 실수가 용납되지 않는 분야의 시스템에 신뢰도를 떨어뜨리는 약점이 존재할 수 있다는 것은 치명적이다. 딥러닝 모델 내부를 해석할 수 있다면 이러한 적대적 공격에 대해 보안 기능을 마련할 수 있을 것이다. 그러나 아직 이러한 기술은 초기 단계에 있어 최근 연구에서도 효과적인 방어법을 내놓지 못하고 있다.<br /></p>

<p>입력 데이터가 신경망 모델을 타고 흐르면서 계속 변환이 일어난다. 각 변환은 입력의 특정 구조에 매우 예민하게 반응한다. 이처럼 모델이 예민하게 반응하는 부분을 공략하여 모델을 헷갈리게 할 수 있다.<br /></p>

<h2 id="13-2-적대적-공격의-정류">13-2. 적대적 공격의 정류</h2>
<p>적대적 공격은 적절한 잡음을 생성하여 사람의 눈에는 똑같이 보이지만 머신러닝 모델을 헷갈리게 만드는 적대적 예제를 생성하는 것이 핵심이다. 인식 오류를 일으키지만 원본과 차이가 가장 적은 잡음을 찾는 것이고, 결국 최적화 문제로 해석할 수 있다. 적대적 공격에선 오차를 줄이기보다는 극대화하는 쪽으로 잡음을 최적화한다.<br /></p>

<p>잡음을 생성하는 방법은 많다. 다만 모델 정보가 필요한지, 우리가 원하는 정답으로 유도할 수 있는지, 여러 모델을 동시에 헷갈리게 할 수 있는지, 학습이 필요한지 등의 여부에 따라 종류가 나뉜다. 극단적으로 이미지 픽셀 하나만 건드려 분류기의 예측을 완전히 빗나가게 할 수도 있다.<br /></p>

<p>적대적 예제에서 잡음의 생성 방법은 분류 기준이 무엇이냐에 따라 여러가지로 나뉜다.<br />
1) 기울기와 같은 모델 정보가 필요한지에 따라, 모델 정보를 토대로 잡음을 생성하는 <strong>화이트박스(White box)</strong> 방법과 모델 정보 없이 생성하는 <strong>블랙박스(Black box)</strong>로 나뉜다.<br />
2) 원하는 정답으로 유도할 수 있다면 <strong>표적(Targeted)</strong>, 아니라면 <strong>비표적(Non-targeted)</strong>으로 분류한다.<br />
3) 잡음을 생성하기 위해 반복된 학습(최적화)이 필요하면 <strong>반복(Iterative)</strong>, 아니면 <strong>원샷(One-shot)</strong>으로 나눌 수 있다.<br />
4) 한 잡음이 특정 입력에만 적용되는지, 모든 이미지에 적용될 수 있는 범용적인 잡음인지로 나눌 수 있다.<br /></p>

<p>가장 강력한 공격 방법은 모델 정보가 필요 없고, 원하는 정답으로 유도할 수 있으며, 복잡한 학습이 필요하지 않고, 여러 모델에 동시 적용할 수 있는 방법이다. 그러나 각 특징에는 기회비용이 존재한다.</p>

<h2 id="13-3-fgsm-공격">13-3. FGSM 공격</h2>
<p class="align-center"><strong>FGSM(Fast gradient sign method)</strong> 란 방법으로 적대적 예제를 생성해, 미리 학습된 딥러닝 모델을 공격해보자. FGSM은 반복된 학습 없이 잡음을 생성하는 원샷 공격이다. 입력 이미지에 대한 기울기의 정보를 추출하여 잡음을 생성한다. 그림 13-1 처럼 잡음이 눈에 보이지 않아야 하므로, 아주 작은 숫자를 잡음에 곱해서 희석한 후 원본 그림에 더한다. 최적화를 통해 더 정교한 잡음을 만들 수도 있다. FGSM은 공격 목표를 정할 수 없는 Non-targeted 방식이자, 대상 모델의 정보가 필요한 화이트박스 방식이다.
<img src="/assets/images/deeplearningpyt/13-1.png" alt="그림 13-1. FGSM 예시" /></p>
<p>그림 13-1. FGSM 예시</p>

<p>이제 공격이 진행되는 방식을 단계별로 보자.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 관련 모듈 임포트
</span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">import</span> <span class="nn">torchvision.models</span> <span class="k">as</span> <span class="n">models</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms</span> <span class="k">as</span> <span class="n">transforms</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
</code></pre></div></div>

<p>토치비전은 AlexNet, VGG, ResNet, SqueezeNet, DenseNEt, Inception 등 여러 가지 학습된 모델을 제공한다. 대부분 이미지넷 데이터셋으로 학습된 모델이다. <code class="language-plaintext highlighter-rouge">models.&lt;모델명&gt;</code> 함수를 호출할 때 인수로 <code class="language-plaintext highlighter-rouge">pretrained=True</code>를 명시하면 학습이 완료된 모델을 사용할 수 있다. 이번 예제에서는 ResNet101을 사용해보자. 정확도를 더 끌어올리고 싶다면 DenseNet이나 Inception v3 같은 모델을 사용하고, 노트북처럼 컴퓨팅 성능이 떨어지는 환경이면 SqueezeNet 같이 가벼운 모델을 사용하자.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 학습된 모델 불러오기
</span><span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">resnet101</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</code></pre></div></div>

<p>위 모델의 예측값은 0~1000까지의 숫자를 뱉는다. 이 값은 이미지넷 데이터셋의 클래스를 가리키는 번호다. <code class="language-plaintext highlighter-rouge">imagenet_classes.json</code> 파일에 숫자와 클래스 제목의 매칭 정보가 담겨 있다. 이 파일을 딕셔너리로 만들어 번호를 레이블 이름으로 변환해주는 idx2class 리스트를 만들고, 이 매칭 정보를 언제든 이용할 수 있도록 하자.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 데이터셋 불러오기
</span><span class="n">CLASSES</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="nb">open</span><span class="p">(</span><span class="s">'./imagenet_samples/imagenet_classes.json'</span><span class="p">))</span>
<span class="n">idx2class</span> <span class="o">=</span> <span class="p">[</span><span class="n">CLASSES</span><span class="p">[</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">)]</span>
</code></pre></div></div>

<p>이제 공격하고자 하는 이미지를 불러오자. 실제 공격은 학습용 데이터에 존재하지 않는 이미지로 가해진다. 우리도 데이터셋에 존재하지 않는 이미지를 새로 준비하자.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 이미지 불러오기
</span><span class="n">img</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="s">'imagenet_samples/corgie.jpg'</span><span class="p">)</span>

<span class="c1"># 이미지를 이미지넷과 같은 크기로 변환후 텐서로 변환
</span><span class="n">img_transforms</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span>
  <span class="n">transforms</span><span class="p">.</span><span class="n">Resize</span><span class="p">((</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">),</span> <span class="n">Image</span><span class="p">.</span><span class="n">BICUBIC</span><span class="p">),</span>
  <span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">(),</span>
<span class="p">])</span>
<span class="n">img_tensor</span> <span class="o">=</span> <span class="n">img_transforms</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="n">img_tensor</span> <span class="o">=</span> <span class="n">img_tensor</span><span class="p">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># 3x224x224 -&gt; 1x3x224x224
</span><span class="k">print</span><span class="p">(</span><span class="s">"이미지 텐서 모양: "</span><span class="p">,</span> <span class="n">img_tensor</span><span class="p">.</span><span class="n">size</span><span class="p">())</span>
</code></pre></div></div>

<p>원본 이미지 텐서를 시각화하기 위해 <code class="language-plaintext highlighter-rouge">squeeze()</code> 함수로 차원을 줄이고, <code class="language-plaintext highlighter-rouge">detach()</code> 함수로 원본 이미지 텐서와의 연결을 끊자. <code class="language-plaintext highlighter-rouge">transpose()</code> 함수로 뒤집힌 이미지를 제자리로 돌려놓고 맷플롯립과 호환되는 넘파이 행렬로 변환하자.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 시각화 위해 넘파이 행렬 변환
# 1x3x224x224 -&gt; 3x224x224
</span><span class="n">original_img_view</span> <span class="o">=</span> <span class="n">img_tensor</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">detach</span><span class="p">()</span>
<span class="n">original_img_view</span> <span class="o">=</span> <span class="n">original_img_view</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">).</span><span class="n">numpy</span><span class="p">()</span> <span class="c1"># transpose는 dim0과 dim1 파라미터 차원을 서로 바꿈
</span><span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">original_img_view</span><span class="p">)</span>
</code></pre></div></div>
<p class="align-center"><img src="/assets/images/deeplearningpyt/13-2.png" alt="그림 13-2. 코드 결과" /></p>
<p>그림 13-2. 코드 결과</p>

<p>```python
output = model(img_tensor)
prediction = output.max(1, keepdim=False)[1]</p>

<p>prediction_idx = prediction.item()
prediction_name = idx2class[prediction_idx]</p>

<p>print(“예측된 레이블 번호: “, prediction_idx)
print(“레이블 이름: “, prediction_name)</p>
:ET