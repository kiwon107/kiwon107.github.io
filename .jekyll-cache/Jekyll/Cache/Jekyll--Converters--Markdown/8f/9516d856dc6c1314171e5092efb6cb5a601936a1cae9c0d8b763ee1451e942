I"*<p>본 포스팅은 “혼자 공부하는 머신러닝+딥러닝” 책 내용을 기반으로 작성되었습니다.
잘못된 내용이 있을 경우 지적해 주시면 감사드리겠습니다.</p>

<h2 id="9-1-lstm-구조">9-1. LSTM 구조</h2>
<p><strong>LSTM(Long Shor-Term Memory)</strong>는 단기 기억을 오래 기억하기 위해 고안되었다. LSTM에는 입력과 가중치를 곱하고 절편을 더해 활성화 함수를 통과시키는 구조를 여러개 가지고 있다. 이런 계산 결과는 다음 타임스텝에 재사용 된다.</p>

<p class="align-center">은닉상태를 먼저 보자. 은닉 상태는 입력과 이전 타임스텝의 은닉 상태를 가중치에 곱한 후 활성화 함수를 통과시켜 다음 은닉 상태를 만든다. 이 때 활성화 함수르 시그모이드 활성화 함수를 사용한다. 또 tanh 활성화 함수를 통과한 어떤 값과 곱해져서 은닉 상태를 만든다. 앞으로 나올 기호 중, 편의상 은닉 상태 계산시 가중치 $w_{x}}$ 와 $w_{h}$ 를 통틀어 $w_{o}$ 라고 하자.
<img src="/assets/images/deeplearningtens/9-1.png" alt="그림 9-1. 코드 결과" /></p>
<p>그림 9-1. 코드 결과</p>

<p>LSTM에는 순환되는 상태가 2개다. 은닉 상태 말고 <strong>셀 상태(Cell state)</strong>라고 부르는 값이 따로 있다. 셀 상태는 다음 층으로 전달되지 않고 LSTM 셀에만 순환되는 값이다.</p>

<h2 id="9-2-lstm-신경망-훈련하기">9-2. LSTM 신경망 훈련하기</h2>

<h2 id="9-3-순환층에-드롭아웃-적용하기">9-3. 순환층에 드롭아웃 적용하기</h2>

<h2 id="9-4-2개의-층을-연결하기">9-4. 2개의 층을 연결하기</h2>

<h2 id="9-5-gru-구조">9-5. GRU 구조</h2>

<h2 id="9-6-gru-신경망-훈련하기">9-6. GRU 신경망 훈련하기</h2>
:ET