I"7<p>본 포스팅은 “혼자 공부하는 머신러닝+딥러닝” 책 내용을 기반으로 작성되었습니다.
잘못된 내용이 있을 경우 지적해 주시면 감사드리겠습니다.</p>

<h2 id="8-1-imdb-리뷰-데이터셋">8-1. IMDB 리뷰 데이터셋</h2>
<p>IMDB 리뷰 데이터셋은 유명한 인터넷 영화 데이터베이스인 imdb.com 에서 수집한 리뷰를 감상평에 따라 긍정과 부정으로 분류해 놓은 데이터셋이다. 참고로 자연어 처리는 컴퓨터를 사용해 인간의 언어를 처리하는 분야인데, 음성 인식, 기계 번역, 감성 분석 등이자연어 처리 분야의 대표적인 세부 분야이다. IMDB 리뷰를 감상평에 따라 분류하는 것은 감성 분석에 해당하며, 자연어 처리 분야에서 훈련 데이터를 종종 <strong>말뭉치(Corpus)</strong>라고 부른다. IMDB 리뷰 데이터셋은 하나의 말뭉치이다.</p>

<p>컴퓨터에서 처리하는 모든 것은 어떤 숫자 데이터이므로, 텍스트 자체를 신경망에 전달하지는 않는다. 텍스트 데이터의 경우 단어를 숫자 데이터로 바꾸는 일반적인 방법은 등장하는 단어마다 고유한 정수를 부여하는 것이다. 예를 들어, ‘He follows the cat. He loves the cat.’ 이라는 문장이 있을 경우 He: 10, follows: 11, the: 12, cat: 13, loves: 14 로 부여하는 것이다. 단어에 매핑되는 정수는 단어의 의미나 크기와 관련이 없다. 이 정수값 사이에는 어떠한 관계도 없다. 일반적으로 영어 문장은 모두 소문자로 바꾸고 구둣점을 삭제한 다음 공백을 기준으로 분리한다. 이렇게 분리된 단어를 <strong>토큰(Token)</strong>이라고 부른다. 하나의 샘플은 여러 개의 토큰으로 이루어져 있고 1개의 토큰이 하나의 타임스탬프에 해당한다. 참고로 한글은 조사가 발달되어 있어 공백으로 나누는 것만으로는 부족하다. 일반적으로 한글은 형태소 분석을 통해 토큰을 만든다.</p>

<p>토큰에 할당하는 정수 중 몇 개는 특정한 용도로 예약되어 있는 경우가 있다. 0: 패딩, 1: 문장의 시작, 2: 어휘 사전에 없는 토큰 이 그 예이다. 훈련 세트에서 고유한 단어를 뽑아 만든 목록을 어휘 사전이라고 하는데, 테스트 세트 안에 어위 사전에 없는 단어가 있다면 2로 변환하여 신경망 모델에 주입한다. 실제 IMDB 리뷰 데이터셋은 영어로 된 문장이지만, 편리하게도 텐서플로에는 이미 정수로 바꾼 데이터가 포함되어 있다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.datasets</span> <span class="kn">import</span> <span class="n">imdb</span>
<span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">),</span> <span class="p">(</span><span class="n">test_input</span><span class="p">,</span> <span class="n">test_target</span><span class="p">)</span> <span class="o">=</span> <span class="n">imdb</span><span class="p">.</span><span class="n">load_data</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">train_input</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">test_input</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(결과) (25000,) (25000,)
</code></pre></div></div>

<p>IMDB 리뷰 텍스트는 길이가 제각각이다. 따라서 고정 크기의 2차원 배열에 담기 보다는 리뷰마다 별도의 파이썬 리스트로 담아야 메모리를 효율적으로 사용할 수 있다. 그렇기 때문에 배열이 1차원인 것이다. 이 데이터는 개별 리뷰를 담은 파이썬 리스트 객체로 이루어진 넘파이 배열이다. 넘파이 배열은 정수나 실수 외에도 파이썬 객체를 담을 수 있다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">train_input</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">train_input</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_input</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">train_input</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(결과) &lt;class 'numpy.ndarray'&gt;
       &lt;class 'list'&gt;
       218
       189
</code></pre></div></div>

<p>보다시피 리뷰마다 각각 길이가 다르다. 여기서는 하나의 리뷰가 하나의 샘플이 된다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">train_input</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(결과) [1, 14, 22, 16, 43, 2, 2, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 2, 112, 50, 2, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 2, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 2, 38, 76, 15, 13, 2, 4, 22, 17, 2, 17, 12, 16, 2, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 2, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 2, 36, 71, 43, 2, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]
</code></pre></div></div>

<p>앞서 설명한대로 IMDB 리뷰 데이터는 이미 정수로 변환되어 있다. 앞서 <code class="language-plaintext highlighter-rouge">num_words=500</code>으로 지정하였기 때문에 어휘 사전에는 500개의 단어만 들어가 있다. 어휘 사전에 없는 단어는 모두 2로 표시되어 나타난다. 500개 선정 기준은 등장 횟수 순서대로 나열한 다음 가장 많이 등장한 500개의 단어를 선택한다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">train_target</span><span class="p">[:</span><span class="mi">20</span><span class="p">])</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(결과) [1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1]
</code></pre></div></div>

<p>타겟 데이터는 리뷰가 긍정인지 부정인지에 대한 정답이 나온다. 0(부정)과 1(긍정)로 나누어진다. 이제 훈련 세트의 20%를 검증 세트로 떼어 놓자.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">train_input</span><span class="p">,</span> <span class="n">val_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">val_target</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">train_target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</code></pre></div></div>

<p>이제 평균적인 리뷰의 길이와 가장 짧은 리뷰의 길이, 그리고 가장 긴 리뷰의 길이를 확인해보자.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">lengths</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">train_input</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">lengths</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">median</span><span class="p">(</span><span class="n">lengths</span><span class="p">))</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(결과) 238.71364 178.0
</code></pre></div></div>

<p>lengths 배열에 대한 히스토그램도 그려보자.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">plt</span><span class="p">.</span><span class="n">hist</span><span class="p">(</span><span class="n">lengths</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'length'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'frequency'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p class="align-center"><img src="/assets/images/deeplearningtens/8-1.png" alt="그림 8-1. 코드 결과" /></p>
<p>그림 8-1. 코드 결과</p>

<p>히스토그램을 보면 대부분의 리뷰 길이가 300 미만이라는 것을 알 수 있다. 평균이 중간보다 높은 이유는 오른쪽 끝에 아주 큰 데이터가 있기 때문이다. 리뷰는 대부분 짧으므로, 중간값보다 훨씬 짧은 100개의 단어만 사용해보자. 물론 여전히 100개 단어보다 작은 리뷰가 있다. 이런 리뷰들의 길이를 100에 맞추도록 패딩이 필요하다. 보통 패딩을 나타내는 토큰으로는 0을 사용한다. 케라스는 시퀀스 데이터 길이를 맞추는 <code class="language-plaintext highlighter-rouge">pad_sequences()</code> 함수를 제공한다. 이 함수로 train_input 길이를 100으로 맞춰보자.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>
<span class="n">train_seq</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">train_input</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">train_seq</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">train_seq</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(결과) (25000, 100)
       [  2  33   6  22  12 215  28  77  52   5  14 407  16  82   2   8   4 107  117   2  15 256   4   2   7   2   5   2  36  71  43   2 476  26 400 317   46   7   4   2   2  13 104  88   4 381  15 297  98  32   2  56  26 141    6 194   2  18   4 226  22  21 134 476  26 480   5 144  30   2  18  51   36  28 224  92  25 104   4 226  65  16  38   2  88  12  16 283   5  16    2 113 103  32  15  16   2  19 178  32]
</code></pre></div></div>

<p>앞에서 확인했을 때 train_input[0]의 길이는 218 이었다. 과연 앞에 118개가 짤렸을까? 뒤에 118개가 짤렸을까?</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">train_input</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="mi">10</span><span class="p">:])</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(결과) [2, 113, 103, 32, 15, 16, 2, 19, 178, 32]
</code></pre></div></div>

:ET