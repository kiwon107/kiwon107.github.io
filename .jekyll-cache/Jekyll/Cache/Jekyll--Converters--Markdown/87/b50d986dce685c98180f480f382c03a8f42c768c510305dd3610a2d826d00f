I"<p>ë³¸ í¬ìŠ¤íŒ…ì€ â€œí­ê·„ë¸Œë¡œì˜ 3ë¶„ ë”¥ëŸ¬ë‹, íŒŒì´í† ì¹˜ë§›â€ ì±… ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.
ì˜ëª»ëœ ë‚´ìš©ì´ ìˆì„ ê²½ìš° ì§€ì í•´ ì£¼ì‹œë©´ ê°ì‚¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤.</p>

<h2 id="7-1-ì»´í“¨í„°ê°€-ë³´ëŠ”-ì´ë¯¸ì§€">7-1. ì»´í“¨í„°ê°€ ë³´ëŠ” ì´ë¯¸ì§€</h2>
<p>ì»´í“¨í„°ì—ì„œ ëª¨ë“  ì´ë¯¸ì§€ëŠ” í”½ì…€ê°’ë“¤ì„ ê°€ë¡œ, ì„¸ë¡œë¡œ ëŠ˜ì–´ë†“ì€ í–‰ë ¬ë¡œ í‘œí˜„ëœë‹¤. ë³´í†µ ì¸ê³µ ì‹ ê²½ë§ì€ ë‹¤ì–‘í•œ í˜•íƒœì˜ ì…ë ¥ì— ëŒ€í•œ í™•ì •ì„±ì´ ë–¨ì–´ì§„ë‹¤. ê°™ì€ ì‹ ë°œ ì´ë¯¸ì§€ë¼ê³  í•´ë„, ì‹ ë°œì´ ì˜†ìœ¼ë¡œ ì¡°ê¸ˆë§Œ ì¹˜ìš°ì³ì§€ë©´ ì˜ˆì¸¡ë¥ ì´ ê¸‰ê²©íˆ ë–¨ì–´ì§„ë‹¤. íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ê°€ì¤‘ì¹˜ê°€ ê°€ìš´ë°ë§Œ ì§‘ì¤‘í•˜ë„ë¡ í•™ìŠµë˜ì—ˆê¸° ë•Œë¬¸ì´ë‹¤. í† ì¹˜ë¹„ì „ì˜ <code class="language-plaintext highlighter-rouge">transforms</code> ë„êµ¬ë¥¼ ì“´ë‹¤í•´ë„, ê³ í™”ì§ˆ ì´ë¯¸ì§€ì˜ í¬ê¸°ë¥¼ ê³ ë ¤í•˜ë©´ ê° ë°°ì¹˜ë‹¹ ì²˜ë¦¬í•´ì•¼í•˜ëŠ” ë°ì´í„° ìˆ˜ëŠ” ì—„ì²­ ëŠ˜ì–´ë‚œë‹¤. ì‹ ë°œì˜ ê°€ìš´ë°, ì™¼ìª½ ë“± ë‹¤ì–‘í•˜ê²Œ ë°°ì¹˜ëœ ì´ë¯¸ì§€ë¥¼ ë”°ë¡œ í•™ìŠµí•˜ëŠ” ê²ƒë„ ì´ìƒì ì´ì§€ ì•Šë‹¤. ì¸ê³µì§€ëŠ¥ì´ í•˜ë‚˜ë¥¼ ë°°ìš°ë©´ì„œ ë‹¤ë¥¸ í•˜ë‚˜ë„ ìì—°ìŠ¤ëŸ½ê²Œ ìœ ì¶”í•˜ë„ë¡ ë§Œë“¤ìˆœ ì—†ì„ê¹Œ?</p>

<h2 id="7-2-ì»¨ë³¼ë£¨ì…˜">7-2. ì»¨ë³¼ë£¨ì…˜</h2>
<p>ì»¨ë³¼ë£¨ì…˜ì˜ ëª©ì ì€ ê³„ì¸¡ì •ìœ¼ë¡œ ì¸ì‹í•  ìˆ˜ ìˆë„ë¡ ë‹¨ê³„ë§ˆë‹¤ ì´ë¯¸ì§€ì˜ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ê²ƒì´ë‹¤. ê° ë‹¨ê³„ì—ì„œëŠ” ì´ë¯¸ì§€ì— ëŒ€í•œ ë‹¤ì–‘í•œ í•„í„°ë¥¼ ì ìš©í•˜ì—¬ ìœ¤ê³½ì„ , ì§ˆê°, í„¸ ë“± ê°ì¢… íŠ¹ì§•ì„ ì¶”ì¶œí•œë‹¤. ìœ¤ê³½ì„ ì„ ê²€ì¶œí•˜ëŠ” í•„í„°ë¥¼ ì‚¬ìš©í•˜ë©´, ë°‘ê·¸ë¦¼ì„ ê·¸ë¦°ê²ƒ ê°™ì€ ì´ë¯¸ì§€ê°€ ì¶œë ¥ëœë‹¤. ì´ëŸ¬í•œ í•„í„°ë¥¼ ì ìš©í•  ë•Œ, ì´ë¯¸ì§€ ì™¼ìª½ ìœ„ì—ì„œ ì˜¤ë¥¸ìª½ ë°‘ê¹Œì§€ ë°€ì–´ê°€ë©° ê³±í•˜ê³  ë”í•˜ëŠ”ë°, ì´ ì‘ì—…ì„ <strong>ì»¨ë³¼ë£¨ì…˜(Convolution)</strong>ì´ë¼ê³  í•œë‹¤. CNNì€ ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí•˜ëŠ” í•„í„°ë¥¼ í•™ìŠµí•œë‹¤. í•„í„°ê°€ í•˜ë‚˜ì˜ ì‘ì€ ì‹ ê²½ë§ì¸ ê²ƒì´ë‹¤.</p>

<h2 id="7-3-cnn-ëª¨ë¸">7-3. CNN ëª¨ë¸</h2>
<p>CNN ëª¨ë¸ì€ ì»¨ë³¼ë£¨ì…˜ ê³„ì¸µ, í’€ë§ ê³„ì¸µ, ì¼ë°˜ì ì¸ ì¸ê³µ ì‹ ê²½ë§ ê³„ì¸µìœ¼ë¡œ êµ¬ì„±ëœë‹¤.</p>
<ul>
  <li>ì»¨ë³¼ë£¨ì…˜ ê³„ì¸µ: ì´ë¯¸ì§€ì˜ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ì—­í• .</li>
  <li>í’€ë§ ê³„ì¸µ: í•„í„°ë¥¼ ê±°ì¹œ ì—¬ëŸ¬ íŠ¹ì§• ì¤‘ ê°€ì¥ ì¤‘ìš”í•œ íŠ¹ì§• í•˜ë‚˜ë¥¼ ê³¨ë¼ëƒ„. ëœ ì¤‘ìš”í•œ íŠ¹ì§•ì„ ë²„ë ¤ ì´ë¯¸ì§€ì˜ ì°¨ì› ê°ì†Œí•¨.</li>
</ul>

<p>ì»¨ë²Œë£¨ì…˜ ì—°ì‚°ì€ ì´ë¯¸ì§€ë¥¼ ê²¹ì¹˜ëŠ” ë§¤ìš° ì‘ì€ ì¡°ê°ìœ¼ë¡œ ìª¼ê°œì–´ í•„í„° ê¸°ëŠ¥ì„ í•˜ëŠ” ì‘ì€ ì‹ ê²½ë§ì— ì ìš©í•œë‹¤. ì´ ì‹ ê²½ë§ì€ ëª¨ë“  ì¡°ê°ì— ë™ì¼í•˜ê²Œ ì ìš©ë˜ë©°, íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê¸° ë•Œë¬¸ì— <strong>ì»¨ë³¼ë£¨ì…˜ í•„í„°</strong> ë˜ëŠ” <strong>ì»¤ë„</strong>ì´ë¼ê³  ë¶€ë¥¸ë‹¤. ë³´í†µ 3 x 3 ë˜ëŠ” 5 x 5 í¬ê¸°ì˜ ì»¤ë„ì´ ì“°ì´ëŠ”ë°, ì»¨ë³¼ë£¨ì…˜ ê³„ì¸µ í•˜ë‚˜ì— ì—¬ëŸ¬ ê°œ ì¡´ì¬í•  ìˆ˜ ìˆë‹¤. í•™ìŠµì´ ì‹œì‘ë˜ë©´, í•„í„° í–‰ë ¬ì˜ ê°’ì€ íŠ¹ì§•ì„ ì˜ ë½‘ì„ ìˆ˜ ìˆë„ë¡ ìµœì í™” ëœë‹¤. ì»¨ë³¼ë£¨ì…˜ì€ ì˜¤ë¥¸ìª½ ì•„ë˜ë¡œ ì›€ì§ì´ë©° ë‹¤ìŒ ì´ë¯¸ì§€ë¥¼ ë§Œë“œëŠ”ë°, í•œ ì¹¸ ë˜ëŠ” ì—¬ëŸ¬ ì¹¸ì„ ê±´ë„ˆë›¸ ìˆ˜ ìˆë‹¤. ì´ ì›€ì§ì„ì„ ì¡°ì ˆí•˜ëŠ” ê°’ì„ <strong>ìŠ¤íŠ¸ë¼ì´ë“œ</strong>ë¼ê³  í•œë‹¤. ìŠ¤íŠ¸ë¼ì´ë“œë¥¼ í¬ê²Œ ì£¼ì–´ ì—¬ëŸ¬ ì¹¸ì„ ê±´ë„ˆë›°ë©´ í…ì„œì˜ í¬ê¸°ëŠ” ì‘ì•„ì§€ê²Œ ëœë‹¤. ì»¨ë³¼ë£¨ì…˜ì„ ê±°ì³ ë§Œë“¤ì–´ì§„ ìƒˆë¡œìš´ ì´ë¯¸ì§€ëŠ” <strong>íŠ¹ì§• ë§µ</strong> ì´ë¼ê³  ë¶€ë¥¸ë‹¤. ì»¨ë³¼ë£¨ì…˜ ê³„ì¸µ ë§ˆë‹¤ ì—¬ëŸ¬ íŠ¹ì§• ë§µì´ ë§Œë“¤ì–´ì§€ê³ , ë‹¤ìŒ ë‹¨ê³„ì¸ í’€ë§ ê³„ì¸µìœ¼ë¡œ ë„˜ì–´ê°€ê²Œ ëœë‹¤. íŠ¹ì§• ë§µ í¬ê¸°ê°€ í¬ë©´ ê³¼ì í•©ì˜ ìœ„í—˜ì´ ì¦ê°€í•˜ë¯€ë¡œ, í’€ë§ ê³„ì¸µì—ì„œ íŠ¹ì§• ë§µì˜ í¬ê¸°ë¥¼ ì¤„ì—¬ì£¼ê¸° ìœ„í•´ íŠ¹ì§•ì„ ê°’ í•˜ë‚˜ë¡œ ì¶”ë ¤ë‚´ì–´ íŠ¹ì§•ì„ ê°•ì¡°í•˜ë„ë¡ í•œë‹¤. ë³´í†µ í•„í„°ê°€ ì§€ë‚˜ê°ˆ ë•Œë§ˆë‹¤ í”½ì…€ì„ ë¬¶ì–´ì„œ í‰ê· ì´ë‚˜ ìµœëŒ€ê°’ì„ ê°€ì ¸ì˜¤ëŠ” ê°„ë‹¨í•œ ì—°ì‚°ì´ ì´ë£¨ì–´ ì§„ë‹¤. íŠ¹ì§• ë§µì„ ê´€ì°°í•˜ë©´, CNN ëª¨ë¸ì´ ì´ë¯¸ì§€ë¥¼ ê³„ì¸µì ìœ¼ë¡œ ì¸ì‹í•œë‹¤ëŠ” ê±³ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. íŠ¹ì§•ì€ ì„  ê°™ì€ ì €ìˆ˜ì¤€ì—ì„œ ëˆˆ, ì½”, ì… ê±°ì³ ì–¼êµ´ ê°™ì€ ê³ ìˆ˜ì¤€ íŠ¹ì§•ì´ ì¶”ì¶œë˜ê²Œ ëœë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, CNNì€ ì‚¬ë¬¼ì´ ì¡°ê¸ˆë§Œ ì¹˜ìš°ì³ì ¸ë„ ì¸ì‹í•˜ì§€ ëª»í•˜ë˜ ì¸ê³µ ì‹ ê²½ë§ì˜ ë¬¸ì œë¥¼ ì´ë¯¸ì§€ ì „ì²´ì— í•„í„°ë¥¼ ì ìš©í•˜ì—¬ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•´ê²°í•´ì¤€ë‹¤.</p>

<h2 id="7-4-cnn-ëª¨ë¸-êµ¬í˜„í•˜ê¸°">7-4. CNN ëª¨ë¸ êµ¬í˜„í•˜ê¸°</h2>
<p>ì´ì œ ì½”ë“œë¡œ êµ¬í˜„í•´ë³´ì. ë°ì´í„°ì…‹ì„ ë§Œë“œëŠ”ê±° ê¹Œì§€ ì­‰ ê°€ë³´ì.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span><span class="p">,</span> <span class="n">datasets</span>

<span class="n">USE_CUDA</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span>
<span class="n">DEVICE</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">'cuda'</span> <span class="k">if</span> <span class="n">USE_CUDA</span> <span class="k">else</span> <span class="s">'cpu'</span><span class="p">)</span>

<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">40</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">64</span>

<span class="n">train_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">(</span>
  <span class="n">datasets</span><span class="p">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="s">'./data'</span><span class="p">,</span>
                  <span class="n">train</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                  <span class="n">download</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                  <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span>
                    <span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                    <span class="n">transforms</span><span class="p">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))</span>
                  <span class="p">])),</span>
  <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">test_loader</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">DataLoader</span><span class="p">(</span>
  <span class="n">datasets</span><span class="p">.</span><span class="n">FashionMNIST</span><span class="p">(</span><span class="s">'./.data'</span><span class="p">,</span>
                  <span class="n">train</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                  <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span>
                    <span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">(),</span>
                    <span class="n">transforms</span><span class="p">.</span><span class="n">Normalize</span><span class="p">((</span><span class="mf">0.1307</span><span class="p">,),</span> <span class="p">(</span><span class="mf">0.3081</span><span class="p">,))</span>
                  <span class="p">])),</span>
  <span class="n">batch_size</span><span class="o">=</span><span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>ì´ì œ CNN ëª¨ë¸ì„ ì„¤ê³„í•  ì°¨ë¡€ë‹¤. <code class="language-plaintext highlighter-rouge">nn.Conv2d</code> í•¨ìˆ˜ë¥¼ í™œìš©í•  ê²ƒì´ë‹¤. ì´ í•¨ìˆ˜ì˜ ì²« ë‘ íŒŒë¼ë¯¸í„°ëŠ” ì…ë ¥ ì±„ë„ ìˆ˜ <code class="language-plaintext highlighter-rouge">in_channels</code>ì™€ ì¶œë ¥ ì±„ë„ ìˆ˜ <code class="language-plaintext highlighter-rouge">out_channels</code> ì´ë‹¤. Fashion MNIST ë°ì´í„°ì…‹ì€ í‘ë°±ì´ë¯¸ì§€ë¼, ìƒ‰ìƒ ì±„ë„ì´ 1ê°œì´ë‹¤. ì²« ì»¨ë³¼ë£¨ì…˜ ê³„ì¸µì—ì„œëŠ” 10ê°œ íŠ¹ì§• ë§µì„ ìƒì„±í•˜ê³ , ë‘ ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜ ê³„ì¸µì€ 10ê°œì˜ íŠ¹ì§• ë§µì„ ë°›ì•„ 20ê°œì˜ íŠ¹ì§• ë§µì„ ë§Œë“¤ë„ë¡ í•´ë³´ì. ì»¤ë„ ì‚¬ì´ì¦ˆëŠ” 5x5ë¡œ ë§Œë“¤ ê²ƒì´ë‹¤. ì»¤ë‚  ì‚¬ì´ì¦ˆì— ìˆ«ë‚˜ í•˜ë‚˜ë§Œ ì§€ì •í•˜ë©´ ì •ì‚¬ê°í˜•ìœ¼ë¡œ ê°„ì£¼í•œë‹¤. (3, 5)ë¡œ ì…ë ¥í•˜ë©´ 3 x 5 í¬ê¸°ì˜ ì§ì‚¬ê°í˜• ì»¤ë„ì„ ë§Œë“¤ìˆ˜ ë„ ìˆë‹¤.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">CNN</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">CNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">conv2d</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">drop</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Dropout2d</span><span class="p">()</span> <span class="c1"># ì´ë²ˆì—ëŠ” functional ëŒ€ì‹  nn.Dropout2D ëª¨ë“ˆ í™œìš©í•´ë´„!
</span>    <span class="bp">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">320</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>  <span class="c1"># ê³„ì¸µì˜ ì¶œë ¥ í¬ê¸°ëŠ” íŠ¹ë³„í•œ ì´ìœ  ì—†ì´ ê³„ì¸µì´ ì§„í–‰ë ìˆ˜ë¡ ì‘ì•„ì§€ë„ë¡ ì„ì˜ë¡œ ì •í•¨.
</span>  
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span>  <span class="c1"># F.max_pool2d() í•¨ìˆ˜ì˜ ë‘ ë²ˆì§¸ ì…ë ¥ì€ ì»¤ë„ í¬ê¸°! í•™ìŠµ íŒŒë¼ë¯¸í„° ë”°ë¡œ ì—†ìŒ.
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">2</span><span class="p">))</span>  <span class="c1"># F.max_pool2d() ì™¸, nn.MaxPool2d ê°™ì€ ì¼ë°˜ ëª¨ë“ˆë„ ì‚¬ìš© ê°€ëŠ¥.
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">320</span><span class="p">)</span>   <span class="c1"># 320ì€ xê°€ ê°€ì§„ ì›ì†Œ ê°œìˆ˜ ì˜ë¯¸
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">drop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>ì´ì œ ë‚˜ë¨¸ì§€ ì½”ë“œë¥¼ ì‘ì„±í•˜ì!</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">CNN</span><span class="p">().</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span><span class="p">):</span>
  <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
  <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">batch_idx</span> <span class="o">%</span> <span class="mi">200</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># batch_idxëŠ” ë°ì´í„°ë¥¼ 64ê°œ ì”© ëª‡ ë²ˆì§¸ ê°€ì ¸ì˜¤ê³  ìˆëŠ”ì§€ë¥¼ ì˜ë¯¸ 
</span>      <span class="k">print</span><span class="p">(</span><span class="s">'Train Epoch: {} [{}/{} ({:.0f}%)]</span><span class="se">\t</span><span class="s">Loss:[:.6f}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">),</span> <span class="mf">100.</span> <span class="o">*</span> <span class="n">batch_idx</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">),</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()))</span>

<span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">):</span>
  <span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
  <span class="n">test_loss</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">correct</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">test_loader</span><span class="p">:</span>
      <span class="n">data</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">),</span> <span class="n">target</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">DEVICE</span><span class="p">)</span>
      <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
      <span class="n">test_loss</span> <span class="o">+=</span> <span class="n">F</span><span class="p">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s">'sum'</span><span class="p">).</span><span class="n">item</span><span class="p">()</span>
      <span class="n">pred</span> <span class="o">=</span> <span class="n">output</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
      <span class="n">correct</span> <span class="o">+=</span> <span class="n">pred</span><span class="p">.</span><span class="n">eq</span><span class="p">(</span><span class="n">target</span><span class="p">.</span><span class="n">view_as</span><span class="p">(</span><span class="n">pred</span><span class="p">)).</span><span class="nb">sum</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>

  <span class="n">test_loss</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">)</span>
  <span class="n">test_accuracy</span> <span class="o">=</span> <span class="mf">100.</span> <span class="o">*</span> <span class="n">correct</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_accuracy</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">EPOCHS</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
  <span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_loader</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
  <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_loader</span><span class="p">)</span>

  <span class="k">print</span><span class="p">(</span><span class="s">'[{}] Test Loss: {:.4f}, Accuracy: {:.2f}%'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">,</span> <span class="n">test_loss</span><span class="p">,</span> <span class="n">test_accuracy</span><span class="p">))</span>
</code></pre></div></div>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(ê²°ê³¼) Train Epoch: 1 [0/60000 (0%)]	Loss:2.300701
       Train Epoch: 1 [12800/60000 (21%)]	Loss:1.112408
       Train Epoch: 1 [25600/60000 (43%)]	Loss:0.926304
       Train Epoch: 1 [38400/60000 (64%)]	Loss:0.991300
       Train Epoch: 1 [51200/60000 (85%)]	Loss:0.779528
       [1] Test Loss: 0.6457, Accuracy: 75.35%
       Train Epoch: 2 [0/60000 (0%)]	Loss:0.681881
       Train Epoch: 2 [12800/60000 (21%)]	Loss:1.025114
       Train Epoch: 2 [25600/60000 (43%)]	Loss:0.503789
       Train Epoch: 2 [38400/60000 (64%)]	Loss:0.849935
       Train Epoch: 2 [51200/60000 (85%)]	Loss:0.603705
       [2] Test Loss: 0.5394, Accuracy: 79.39%
       Train Epoch: 3 [0/60000 (0%)]	Loss:0.586898
       Train Epoch: 3 [12800/60000 (21%)]	Loss:0.700720
       Train Epoch: 3 [25600/60000 (43%)]	Loss:0.524979
       Train Epoch: 3 [38400/60000 (64%)]	Loss:0.359380
       Train Epoch: 3 [51200/60000 (85%)]	Loss:0.585687
       [3] Test Loss: 0.4953, Accuracy: 82.11%
       Train Epoch: 4 [0/60000 (0%)]	Loss:0.602041
       Train Epoch: 4 [12800/60000 (21%)]	Loss:0.539423
       Train Epoch: 4 [25600/60000 (43%)]	Loss:0.510981
       Train Epoch: 4 [38400/60000 (64%)]	Loss:0.637181
       Train Epoch: 4 [51200/60000 (85%)]	Loss:0.361220
       [4] Test Loss: 0.4463, Accuracy: 84.13%
       Train Epoch: 5 [0/60000 (0%)]	Loss:0.422438
       Train Epoch: 5 [12800/60000 (21%)]	Loss:0.506737
       Train Epoch: 5 [25600/60000 (43%)]	Loss:0.485518
       Train Epoch: 5 [38400/60000 (64%)]	Loss:0.550422
       Train Epoch: 5 [51200/60000 (85%)]	Loss:0.439864
       [5] Test Loss: 0.4365, Accuracy: 84.02%
       Train Epoch: 6 [0/60000 (0%)]	Loss:0.393532
       Train Epoch: 6 [12800/60000 (21%)]	Loss:0.376332
       Train Epoch: 6 [25600/60000 (43%)]	Loss:0.503539
       Train Epoch: 6 [38400/60000 (64%)]	Loss:0.585389
       Train Epoch: 6 [51200/60000 (85%)]	Loss:0.457623
       [6] Test Loss: 0.4253, Accuracy: 84.39%
       Train Epoch: 7 [0/60000 (0%)]	Loss:0.435238
       Train Epoch: 7 [12800/60000 (21%)]	Loss:0.433837
       Train Epoch: 7 [25600/60000 (43%)]	Loss:0.486959
       Train Epoch: 7 [38400/60000 (64%)]	Loss:0.519784
       Train Epoch: 7 [51200/60000 (85%)]	Loss:0.569560
       [7] Test Loss: 0.3837, Accuracy: 86.07%
       Train Epoch: 8 [0/60000 (0%)]	Loss:0.455356
       Train Epoch: 8 [12800/60000 (21%)]	Loss:0.455740
       Train Epoch: 8 [25600/60000 (43%)]	Loss:0.469353
       Train Epoch: 8 [38400/60000 (64%)]	Loss:0.404051
       Train Epoch: 8 [51200/60000 (85%)]	Loss:0.565929
       [8] Test Loss: 0.3713, Accuracy: 86.37%
       Train Epoch: 9 [0/60000 (0%)]	Loss:0.503008
       Train Epoch: 9 [12800/60000 (21%)]	Loss:0.410055
       Train Epoch: 9 [25600/60000 (43%)]	Loss:0.370442
       Train Epoch: 9 [38400/60000 (64%)]	Loss:0.350443
       Train Epoch: 9 [51200/60000 (85%)]	Loss:0.417547
       [9] Test Loss: 0.3639, Accuracy: 86.78%
       Train Epoch: 10 [0/60000 (0%)]	Loss:0.524522
       Train Epoch: 10 [12800/60000 (21%)]	Loss:0.404944
       Train Epoch: 10 [25600/60000 (43%)]	Loss:0.401919
       Train Epoch: 10 [38400/60000 (64%)]	Loss:0.335519
       Train Epoch: 10 [51200/60000 (85%)]	Loss:0.444212
       [10] Test Loss: 0.3506, Accuracy: 87.15%
       Train Epoch: 11 [0/60000 (0%)]	Loss:0.380572
       Train Epoch: 11 [12800/60000 (21%)]	Loss:0.369728
       Train Epoch: 11 [25600/60000 (43%)]	Loss:0.360946
       Train Epoch: 11 [38400/60000 (64%)]	Loss:0.324107
       Train Epoch: 11 [51200/60000 (85%)]	Loss:0.303930
       [11] Test Loss: 0.3568, Accuracy: 87.11%
       Train Epoch: 12 [0/60000 (0%)]	Loss:0.354425
       Train Epoch: 12 [12800/60000 (21%)]	Loss:0.275802
       Train Epoch: 12 [25600/60000 (43%)]	Loss:0.262319
       Train Epoch: 12 [38400/60000 (64%)]	Loss:0.401320
       Train Epoch: 12 [51200/60000 (85%)]	Loss:0.400510
       [12] Test Loss: 0.3445, Accuracy: 87.65%
       Train Epoch: 13 [0/60000 (0%)]	Loss:0.301611
       Train Epoch: 13 [12800/60000 (21%)]	Loss:0.367334
       Train Epoch: 13 [25600/60000 (43%)]	Loss:0.386293
       Train Epoch: 13 [38400/60000 (64%)]	Loss:0.497484
       Train Epoch: 13 [51200/60000 (85%)]	Loss:0.377190
       [13] Test Loss: 0.3377, Accuracy: 87.51%
       Train Epoch: 14 [0/60000 (0%)]	Loss:0.550325
       Train Epoch: 14 [12800/60000 (21%)]	Loss:0.772415
       Train Epoch: 14 [25600/60000 (43%)]	Loss:0.183556
       Train Epoch: 14 [38400/60000 (64%)]	Loss:0.431902
       Train Epoch: 14 [51200/60000 (85%)]	Loss:0.437415
       [14] Test Loss: 0.3332, Accuracy: 87.43%
       Train Epoch: 15 [0/60000 (0%)]	Loss:0.409025
       Train Epoch: 15 [12800/60000 (21%)]	Loss:0.276985
       Train Epoch: 15 [25600/60000 (43%)]	Loss:0.340309
       Train Epoch: 15 [38400/60000 (64%)]	Loss:0.256900
       Train Epoch: 15 [51200/60000 (85%)]	Loss:0.520286
       [15] Test Loss: 0.3280, Accuracy: 88.42%
       Train Epoch: 16 [0/60000 (0%)]	Loss:0.272302
       Train Epoch: 16 [12800/60000 (21%)]	Loss:0.352614
       Train Epoch: 16 [25600/60000 (43%)]	Loss:0.258878
       Train Epoch: 16 [38400/60000 (64%)]	Loss:0.311896
       Train Epoch: 16 [51200/60000 (85%)]	Loss:0.217881
       [16] Test Loss: 0.3239, Accuracy: 88.32%
       Train Epoch: 17 [0/60000 (0%)]	Loss:0.213851
       Train Epoch: 17 [12800/60000 (21%)]	Loss:0.331738
       Train Epoch: 17 [25600/60000 (43%)]	Loss:0.336340
       Train Epoch: 17 [38400/60000 (64%)]	Loss:0.360911
       Train Epoch: 17 [51200/60000 (85%)]	Loss:0.397059
       [17] Test Loss: 0.3235, Accuracy: 88.61%
       Train Epoch: 18 [0/60000 (0%)]	Loss:0.320855
       Train Epoch: 18 [12800/60000 (21%)]	Loss:0.526109
       Train Epoch: 18 [25600/60000 (43%)]	Loss:0.367768
       Train Epoch: 18 [38400/60000 (64%)]	Loss:0.395585
       Train Epoch: 18 [51200/60000 (85%)]	Loss:0.387897
       [18] Test Loss: 0.3190, Accuracy: 88.46%
       Train Epoch: 19 [0/60000 (0%)]	Loss:0.360296
       Train Epoch: 19 [12800/60000 (21%)]	Loss:0.220974
       Train Epoch: 19 [25600/60000 (43%)]	Loss:0.460590
       Train Epoch: 19 [38400/60000 (64%)]	Loss:0.380125
       Train Epoch: 19 [51200/60000 (85%)]	Loss:0.571719
       [19] Test Loss: 0.3211, Accuracy: 88.53%
       Train Epoch: 20 [0/60000 (0%)]	Loss:0.357328
       Train Epoch: 20 [12800/60000 (21%)]	Loss:0.322553
       Train Epoch: 20 [25600/60000 (43%)]	Loss:0.274892
       Train Epoch: 20 [38400/60000 (64%)]	Loss:0.351856
       Train Epoch: 20 [51200/60000 (85%)]	Loss:0.244439
       [20] Test Loss: 0.3187, Accuracy: 88.36%
       Train Epoch: 21 [0/60000 (0%)]	Loss:0.346970
       Train Epoch: 21 [12800/60000 (21%)]	Loss:0.232282
       Train Epoch: 21 [25600/60000 (43%)]	Loss:0.357492
       Train Epoch: 21 [38400/60000 (64%)]	Loss:0.303289
       Train Epoch: 21 [51200/60000 (85%)]	Loss:0.310551
       [21] Test Loss: 0.3088, Accuracy: 88.88%
       Train Epoch: 22 [0/60000 (0%)]	Loss:0.329502
       Train Epoch: 22 [12800/60000 (21%)]	Loss:0.387265
       Train Epoch: 22 [25600/60000 (43%)]	Loss:0.338717
       Train Epoch: 22 [38400/60000 (64%)]	Loss:0.266329
       Train Epoch: 22 [51200/60000 (85%)]	Loss:0.335189
       [22] Test Loss: 0.3150, Accuracy: 89.05%
       Train Epoch: 23 [0/60000 (0%)]	Loss:0.159057
       Train Epoch: 23 [12800/60000 (21%)]	Loss:0.271390
       Train Epoch: 23 [25600/60000 (43%)]	Loss:0.552502
       Train Epoch: 23 [38400/60000 (64%)]	Loss:0.198685
       Train Epoch: 23 [51200/60000 (85%)]	Loss:0.404185
       [23] Test Loss: 0.3021, Accuracy: 89.17%
       Train Epoch: 24 [0/60000 (0%)]	Loss:0.413641
       Train Epoch: 24 [12800/60000 (21%)]	Loss:0.276917
       Train Epoch: 24 [25600/60000 (43%)]	Loss:0.258801
       Train Epoch: 24 [38400/60000 (64%)]	Loss:0.326398
       Train Epoch: 24 [51200/60000 (85%)]	Loss:0.304481
       [24] Test Loss: 0.3031, Accuracy: 89.44%
       Train Epoch: 25 [0/60000 (0%)]	Loss:0.235821
       Train Epoch: 25 [12800/60000 (21%)]	Loss:0.262151
       Train Epoch: 25 [25600/60000 (43%)]	Loss:0.510176
       Train Epoch: 25 [38400/60000 (64%)]	Loss:0.340141
       Train Epoch: 25 [51200/60000 (85%)]	Loss:0.411650
       [25] Test Loss: 0.3058, Accuracy: 89.06%
       Train Epoch: 26 [0/60000 (0%)]	Loss:0.334781
       Train Epoch: 26 [12800/60000 (21%)]	Loss:0.193650
       Train Epoch: 26 [25600/60000 (43%)]	Loss:0.358560
       Train Epoch: 26 [38400/60000 (64%)]	Loss:0.371106
       Train Epoch: 26 [51200/60000 (85%)]	Loss:0.309762
       [26] Test Loss: 0.3021, Accuracy: 89.41%
       Train Epoch: 27 [0/60000 (0%)]	Loss:0.204864
       Train Epoch: 27 [12800/60000 (21%)]	Loss:0.322649
       Train Epoch: 27 [25600/60000 (43%)]	Loss:0.445778
       Train Epoch: 27 [38400/60000 (64%)]	Loss:0.298792
       Train Epoch: 27 [51200/60000 (85%)]	Loss:0.291063
       [27] Test Loss: 0.2998, Accuracy: 89.46%
       Train Epoch: 28 [0/60000 (0%)]	Loss:0.383044
       Train Epoch: 28 [12800/60000 (21%)]	Loss:0.274902
       Train Epoch: 28 [25600/60000 (43%)]	Loss:0.391820
       Train Epoch: 28 [38400/60000 (64%)]	Loss:0.200719
       Train Epoch: 28 [51200/60000 (85%)]	Loss:0.276480
       [28] Test Loss: 0.2967, Accuracy: 89.52%
       Train Epoch: 29 [0/60000 (0%)]	Loss:0.202882
       Train Epoch: 29 [12800/60000 (21%)]	Loss:0.411317
       Train Epoch: 29 [25600/60000 (43%)]	Loss:0.227023
       Train Epoch: 29 [38400/60000 (64%)]	Loss:0.210601
       Train Epoch: 29 [51200/60000 (85%)]	Loss:0.456766
       [29] Test Loss: 0.2920, Accuracy: 89.73%
       Train Epoch: 30 [0/60000 (0%)]	Loss:0.332740
       Train Epoch: 30 [12800/60000 (21%)]	Loss:0.332109
       Train Epoch: 30 [25600/60000 (43%)]	Loss:0.271895
       Train Epoch: 30 [38400/60000 (64%)]	Loss:0.314486
       Train Epoch: 30 [51200/60000 (85%)]	Loss:0.311733
       [30] Test Loss: 0.2969, Accuracy: 89.70%
       Train Epoch: 31 [0/60000 (0%)]	Loss:0.235440
       Train Epoch: 31 [12800/60000 (21%)]	Loss:0.640069
       Train Epoch: 31 [25600/60000 (43%)]	Loss:0.236893
       Train Epoch: 31 [38400/60000 (64%)]	Loss:0.327169
       Train Epoch: 31 [51200/60000 (85%)]	Loss:0.312421
       [31] Test Loss: 0.3071, Accuracy: 88.92%
       Train Epoch: 32 [0/60000 (0%)]	Loss:0.390608
       Train Epoch: 32 [12800/60000 (21%)]	Loss:0.236806
       Train Epoch: 32 [25600/60000 (43%)]	Loss:0.349514
       Train Epoch: 32 [38400/60000 (64%)]	Loss:0.214653
       Train Epoch: 32 [51200/60000 (85%)]	Loss:0.337280
       [32] Test Loss: 0.2966, Accuracy: 89.66%
       Train Epoch: 33 [0/60000 (0%)]	Loss:0.307142
       Train Epoch: 33 [12800/60000 (21%)]	Loss:0.460213
       Train Epoch: 33 [25600/60000 (43%)]	Loss:0.291281
       Train Epoch: 33 [38400/60000 (64%)]	Loss:0.190798
       Train Epoch: 33 [51200/60000 (85%)]	Loss:0.233088
       [33] Test Loss: 0.2984, Accuracy: 89.41%
       Train Epoch: 34 [0/60000 (0%)]	Loss:0.248791
       Train Epoch: 34 [12800/60000 (21%)]	Loss:0.194296
       Train Epoch: 34 [25600/60000 (43%)]	Loss:0.326523
       Train Epoch: 34 [38400/60000 (64%)]	Loss:0.330689
       Train Epoch: 34 [51200/60000 (85%)]	Loss:0.250794
       [34] Test Loss: 0.2986, Accuracy: 89.85%
       Train Epoch: 35 [0/60000 (0%)]	Loss:0.236275
       Train Epoch: 35 [12800/60000 (21%)]	Loss:0.222249
       Train Epoch: 35 [25600/60000 (43%)]	Loss:0.224169
       Train Epoch: 35 [38400/60000 (64%)]	Loss:0.373505
       Train Epoch: 35 [51200/60000 (85%)]	Loss:0.217376
       [35] Test Loss: 0.2929, Accuracy: 89.62%
       Train Epoch: 36 [0/60000 (0%)]	Loss:0.364806
       Train Epoch: 36 [12800/60000 (21%)]	Loss:0.143291
       Train Epoch: 36 [25600/60000 (43%)]	Loss:0.279793
       Train Epoch: 36 [38400/60000 (64%)]	Loss:0.308664
       Train Epoch: 36 [51200/60000 (85%)]	Loss:0.414453
       [36] Test Loss: 0.2994, Accuracy: 89.41%
       Train Epoch: 37 [0/60000 (0%)]	Loss:0.414704
       Train Epoch: 37 [12800/60000 (21%)]	Loss:0.214920
       Train Epoch: 37 [25600/60000 (43%)]	Loss:0.283646
       Train Epoch: 37 [38400/60000 (64%)]	Loss:0.281922
       Train Epoch: 37 [51200/60000 (85%)]	Loss:0.456622
       [37] Test Loss: 0.2933, Accuracy: 89.71%
       Train Epoch: 38 [0/60000 (0%)]	Loss:0.307230
       Train Epoch: 38 [12800/60000 (21%)]	Loss:0.317103
       Train Epoch: 38 [25600/60000 (43%)]	Loss:0.231398
       Train Epoch: 38 [38400/60000 (64%)]	Loss:0.152353
       Train Epoch: 38 [51200/60000 (85%)]	Loss:0.440910
       [38] Test Loss: 0.3055, Accuracy: 89.42%
       Train Epoch: 39 [0/60000 (0%)]	Loss:0.309281
       Train Epoch: 39 [12800/60000 (21%)]	Loss:0.238020
       Train Epoch: 39 [25600/60000 (43%)]	Loss:0.270904
       Train Epoch: 39 [38400/60000 (64%)]	Loss:0.623173
       Train Epoch: 39 [51200/60000 (85%)]	Loss:0.150122
       [39] Test Loss: 0.3086, Accuracy: 89.04%
       Train Epoch: 40 [0/60000 (0%)]	Loss:0.231346
       Train Epoch: 40 [12800/60000 (21%)]	Loss:0.226066
       Train Epoch: 40 [25600/60000 (43%)]	Loss:0.247825
       Train Epoch: 40 [38400/60000 (64%)]	Loss:0.190665
       Train Epoch: 40 [51200/60000 (85%)]	Loss:0.155920
       [40] Test Loss: 0.2999, Accuracy: 89.65%
</code></pre></div></div>
:ET