I"XZ<p>본 포스팅은 “펭귄브로의 3분 딥러닝, 파이토치맛” 책 내용을 기반으로 작성되었습니다.
잘못된 내용이 있을 경우 지적해 주시면 감사드리겠습니다.</p>

<h2 id="3-1-인공-신겸망ann">3-1. 인공 신겸망(ANN)</h2>
<p>인공 신경망(Artificial Neural Network)는 인간의 뇌 혹은 신경계의 작동 방식에서 영감을 받았다.<br /></p>
<ul>
  <li>입력층: 인공 신경망에서 자극을 입력받는 감각기관에 해당하는 부분</li>
  <li>은닉층: 입력층을 거친 자극을 처리해 다음 은닉층(인접한 신경세포)로 전달하는 부분. 이렇게 여러 은닉층을 거쳐 자극이 처리되다 보면, 자극에 따라 다양한 반응을 보이게 됨.</li>
  <li>출력층: 은닉층을 거쳐 처리된 자극이 거치는 마지막 뉴런.</li>
  <li>노드: 각 층에 존재하는 한 단위의 인공뉴런</li>
</ul>

<p>하나의 생물학적 신경세포는 인접한 신경세포로 자극을 전달하기 전, 입력받은 자극에 여러 화학적 처리를 가함. 이와 비슷하게 인공 신경망도 가중치와 편향을 이용하여 데이터를 처리한다.</p>
<ul>
  <li>가중치: 입력 신호가 출력에 주는 영향을 계산하는 매개변수</li>
  <li>편향: 노드가 얼마나 데이터에 민감한지 알려주는 매개변수</li>
  <li>활성화 함수: 입력에 적절한 처리를 하여 출력 신호로 변환하는 함수. 입력 신호의 합이 활성화를 일으키는지 아닌지를 정의. 즉 다음 뉴런으로 자극(데이터)을 어느정도 활성화시켜 전달할지를 알려줌!</li>
</ul>

<p>각 층마다 가중치 곱과 활성화 함수를 거치고, 이렇게 층 간 자극 처리와 전달 과정을 몇 겹 걸쳐 반복한 후 마지막 출력층에서 결과값을 만들어내는 것이 인공 신경망의 기본적인 작동 원리이다. 그 다음, 인공 신경망의 출력층이 낸 결과값과 정답을 비교하여 오차를 계산한다. 이 오차를 기반으로 경사하강법을 활용해 출력층의 가중치부터 입력층의 가중치까지 모두 변경해준다. 이렇게 전체 층의 가중치를 뒤에서부터 차례대로 조정하고 최적화하는 알고리즘이 바로 <strong>역전파 알고리즘</strong>이다.</p>

<h2 id="3-2-긴딘힌-분류-모델-구현하기">3-2. 긴딘힌 분류 모델 구현하기</h2>
<p>지도학습 중 분류를 하는 간단한 ANN을 만들어보자. 넘파이, 사이킷런, 맷플롯립을 임프트 할 것이다.</p>
<ul>
  <li>넘파이: 유명한 수치해석용 라이브러리. 행렬과 벡터 연산에 유용. 파이토치도 이 넘파이를 활용하여 개발됨.</li>
  <li>사이킷런: 파이썬의 대표적인 머신러닝 라이브러리. 딥러닝을 제외한 머신러닝은 거의 이 라이브러리 쓴다 봐도 무방.</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">numpy</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
</code></pre></div></div>

<p>먼저 신경망 학습과 평가에 사용할 데이터셋을 만든다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_dim</span> <span class="o">=</span> <span class="mi">2</span>
<span class="c1"># 2차원 벡터 형태로 4개의 클러스터 갖는 데이터 만듬. 각 데이터는 0, 1, 2, 3으로 인덱싱 됨.
</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">80</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="n">n_dim</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]],</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="n">n_dim</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]],</span> <span class="n">shuffle</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># 0번과 1번 레이블 갖는 데이터는 전부 0번, 2번과 3번 레이블 갖는 데이터는 전부 1번
</span><span class="k">def</span> <span class="nf">label_map</span><span class="p">(</span><span class="n">y_</span><span class="p">,</span> <span class="n">from_</span><span class="p">,</span> <span class="n">to_</span><span class="p">):</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">numpy</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="n">y_</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">f</span> <span class="ow">in</span> <span class="n">from_</span><span class="p">:</span>
    <span class="n">y</span><span class="p">[</span><span class="n">y_</span> <span class="o">==</span> <span class="n">f</span><span class="p">]</span> <span class="o">=</span> <span class="n">to_</span>
  <span class="k">return</span> <span class="n">y</span>

<span class="n">y_train</span> <span class="o">=</span> <span class="n">label_map</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">label_map</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">label_map</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">label_map</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>데이터가 잘 만들어졌는지 시각화 해보자.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">vis_data</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s">'r'</span><span class="p">):</span>
  <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
    <span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">x_</span><span class="p">,</span> <span class="n">y_</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">y_</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
      <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s">'*'</span><span class="p">,</span> <span class="n">marketfacecolor</span><span class="o">=</span><span class="s">'none'</span><span class="p">,</span> <span class="n">markeredgecolor</span><span class="o">=</span><span class="n">c</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
      <span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x_</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">+</span><span class="s">'o'</span> <span class="k">if</span> <span class="n">y_</span><span class="o">==</span><span class="mi">0</span> <span class="k">else</span> <span class="n">c</span><span class="o">+</span><span class="s">'+'</span><span class="p">)</span>
  
<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">vis_data</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s">'r'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>
<p class="align-center"><img src="/assets/images/deeplearningpyt/3-1.png" alt="그림 3-1. 코드 결과" /></p>
<p>그림 3-1. 코드 결과</p>

<p>데이터가 잘 생성된 것으로 보인다. 이제 넘파이 벡터 형식의 데이터들을 파이토치 텐서로 바꿔주자.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x_train</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
<span class="n">x_test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span>
</code></pre></div></div>

<p>이제 신경망 모델을 만들자. 파이토치에서 신경망은 신경망 모듈(torch.nn.Module)을 상속받는 파이썬 클래스로 정의한다. nn.Module을 상속받으면 파이토치 프레임워크에 있는 각종 도구를 쉽게 적용할 수 있다.<br />
신경망의 구조와 동작을 정의하는 생성자를 모델 클래스에 먼저 정의한다. <code class="language-plaintext highlighter-rouge">NeuralNet</code>클래스의 객체를 만들 때 <code class="language-plaintext highlighter-rouge">input_size</code>와 <code class="language-plaintext highlighter-rouge">hidden_size</code>를 입력받도록 정의한다. <code class="language-plaintext highlighter-rouge">input_size</code>는 신경망에 입력되는 데이터의 차원이다.<br />
다음 입력된 데이터가 인공 신경망 통과하면서 거치는 연산들을 정의한다. <code class="language-plaintext highlighter-rouge">torch.nn.Linear()</code>함수는 행렬곱과 편향을 포함한 연산을 지원하는 객체를 반환한다. <code class="language-plaintext highlighter-rouge">linear_1</code>과 <code class="language-plaintext highlighter-rouge">linear_2</code>객체는 나중에 함수로 쓰일 수 있다. <code class="language-plaintext highlighter-rouge">reul()</code>와 <code class="language-plaintext highlighter-rouge">sigmoid()</code>는 각 단계에서 수행할 활성화 함수이다.<br />
마지막으로 생성자 <code class="language-plaintext highlighter-rouge">__init__()</code>에서 정의한 동작들을 차례대로 실행하는 <code class="language-plaintext highlighter-rouge">forward()</code> 함수를 구현한다. <code class="language-plaintext highlighter-rouge">linear_1</code>은 입력 데이터에 <code class="language-plaintext highlighter-rouge">[input_size, hidden_size]</code> 크기의 가중치를 행렬곱하고 편향을 더하여 <code class="language-plaintext highlighter-rouge">[1, hidden_size]</code>꼴의 텐서를 반환한다. 이 텐서에 <code class="language-plaintext highlighter-rouge">relu()</code>함수를 적용하여 0보다 작으면 0을, 0보다 크면 입력값을 그대로 출력하도록 한다. 그 다음 다시 <code class="language-plaintext highlighter-rouge">linear_2</code> 함수를 거쳐 <code class="language-plaintext highlighter-rouge">[1,1]</code> 꼴의 텐서를 반환한다. 이 텐서를 <code class="language-plaintext highlighter-rouge">sigmoid()</code> 거쳐 0과 1사이의 확률값으로 변환되도록 한다. 0애 가까우면 클래스 0, 1에 가까우면 클래스 1이 반환될 것이다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">NeuralNet</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">NeuralNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="p">.</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">input_size</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
    
    <span class="bp">self</span><span class="p">.</span><span class="n">linear_1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">input_size</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">linear_2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">sigmoid</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Sigmoid</span><span class="p">()</span>
  
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">):</span>
    <span class="n">linear1</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">linear_1</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span> <span class="c1"># __call__ 함수로 구현하면 해당 객체 호출하여 데이터 입력시 출력값 리턴 가능하다!
</span>    <span class="n">relu</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="n">linear1</span><span class="p">)</span>
    <span class="n">linear2</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">linear_2</span><span class="p">(</span><span class="n">relu</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">linear2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div>

<p>이제 신경망 객체 생성 후 학습에 필요한 여러 변수와 알고리즘을 정의한다.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">NeuralNet</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.03</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">BCELoss</span><span class="p">()</span>
<span class="n">epoch</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
</code></pre></div></div>

<p>criterion은 여러 오차 함수 중 어떤 함수를 쓸 것인지에 대한 변수로, 여기서는 이진 교차 엔트로피인 <code class="language-plaintext highlighter-rouge">BCELoss()</code> 함수를 사용한다. 에폭은 전체 학습 데이터를 총 몇 번 모델에 입력할지 결정하는 변수이다. 에폭은 너무 작게 설정하면 모델이 충분히 학습되지 않을 수 있으며, 크게 설정하면 학습이 오래걸린다. 학습에 사용할 최적화 알고리즘은 확률적 경사하강법(SGD)를 선택할 것이다. <code class="language-plaintext highlighter-rouge">optimizer</code>는 <code class="language-plaintext highlighter-rouge">step()</code> 함수를 부를 때 마다 가중치를 학습률만큼 갱신한다. 그래서 <code class="language-plaintext highlighter-rouge">moel.parameter()</code> 함수로 모델 내부의 가중치를 <code class="language-plaintext highlighter-rouge">SGD()</code> 함수에 입력하고 학습률도 <code class="language-plaintext highlighter-rouge">SGD()</code> 함수에 입력했다.<br /></p>

<p>이제 아직 학습하지 않은 모델의 성능을 보자.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
<span class="n">test_loss_before</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">x_test</span><span class="p">).</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">y_test</span><span class="p">)</span>  <span class="c1"># 모델의 결과값과 레이블값의 차원을 맞추기 위해 squeeze() 함수 사용
</span><span class="k">print</span><span class="p">(</span><span class="s">'Before Training, test loss is {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">test_loss_before</span><span class="p">.</span><span class="n">item</span><span class="p">()))</span>
</code></pre></div></div>
:ET